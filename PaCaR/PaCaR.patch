diff --git a/arch/s390/kernel/uv.c b/arch/s390/kernel/uv.c
index 9646f773208a..373b483ebfd5 100644
--- a/arch/s390/kernel/uv.c
+++ b/arch/s390/kernel/uv.c
@@ -231,7 +231,7 @@ static int expected_folio_refs(struct folio *folio)
 		res++;
 	} else if (folio_mapping(folio)) {
 		res++;
-		if (folio->private)
+		if (folio_get_private(folio))
 			res++;
 	}
 	return res;
diff --git a/fs/btrfs/defrag.c b/fs/btrfs/defrag.c
index 968dae953948..0bf3aa3fc934 100644
--- a/fs/btrfs/defrag.c
+++ b/fs/btrfs/defrag.c
@@ -909,7 +909,7 @@ static struct folio *defrag_prepare_one_folio(struct btrfs_inode *inode, pgoff_t
 		 * We unlocked the folio above, so we need check if it was
 		 * released or not.
 		 */
-		if (folio->mapping != mapping || !folio->private) {
+		if (folio->mapping != mapping || !folio_get_private(folio)) {
 			folio_unlock(folio);
 			folio_put(folio);
 			goto again;
@@ -923,7 +923,7 @@ static struct folio *defrag_prepare_one_folio(struct btrfs_inode *inode, pgoff_t
 	if (!folio_test_uptodate(folio)) {
 		btrfs_read_folio(NULL, folio);
 		folio_lock(folio);
-		if (folio->mapping != mapping || !folio->private) {
+		if (folio->mapping != mapping || !folio_get_private(folio)) {
 			folio_unlock(folio);
 			folio_put(folio);
 			goto again;
diff --git a/fs/buffer.c b/fs/buffer.c
index 32bd0f4c4223..50ddbe7e7c28 100644
--- a/fs/buffer.c
+++ b/fs/buffer.c
@@ -19,6 +19,11 @@
  * async buffer flushing, 1999 Andrea Arcangeli <andrea@suse.de>
  */
 
+#include "linux/duplication.h"
+#include "linux/mm_types.h"
+#include "linux/page-flags.h"
+#include "linux/pagemap.h"
+#include "linux/stddef.h"
 #include <linux/kernel.h>
 #include <linux/sched/signal.h>
 #include <linux/syscalls.h>
diff --git a/fs/iomap/buffered-io.c b/fs/iomap/buffered-io.c
index 1bad460275eb..3b82d060c6fe 100644
--- a/fs/iomap/buffered-io.c
+++ b/fs/iomap/buffered-io.c
@@ -3,6 +3,7 @@
  * Copyright (C) 2010 Red Hat, Inc.
  * Copyright (C) 2016-2023 Christoph Hellwig.
  */
+#include "linux/duplication.h"
 #include <linux/module.h>
 #include <linux/compiler.h>
 #include <linux/fs.h>
@@ -71,7 +72,7 @@ static bool ifs_set_range_uptodate(struct folio *folio,
 static void iomap_set_range_uptodate(struct folio *folio, size_t off,
 		size_t len)
 {
-	struct iomap_folio_state *ifs = folio->private;
+	struct iomap_folio_state *ifs = folio_get_private(folio);
 	unsigned long flags;
 	bool uptodate = true;
 
@@ -122,7 +123,7 @@ static unsigned ifs_find_dirty_range(struct folio *folio,
 static unsigned iomap_find_dirty_range(struct folio *folio, u64 *range_start,
 		u64 range_end)
 {
-	struct iomap_folio_state *ifs = folio->private;
+	struct iomap_folio_state *ifs = folio_get_private(folio);
 
 	if (*range_start >= range_end)
 		return 0;
@@ -149,7 +150,7 @@ static void ifs_clear_range_dirty(struct folio *folio,
 
 static void iomap_clear_range_dirty(struct folio *folio, size_t off, size_t len)
 {
-	struct iomap_folio_state *ifs = folio->private;
+	struct iomap_folio_state *ifs = folio_get_private(folio);
 
 	if (ifs)
 		ifs_clear_range_dirty(folio, ifs, off, len);
@@ -172,7 +173,7 @@ static void ifs_set_range_dirty(struct folio *folio,
 
 static void iomap_set_range_dirty(struct folio *folio, size_t off, size_t len)
 {
-	struct iomap_folio_state *ifs = folio->private;
+	struct iomap_folio_state *ifs = folio_get_private(folio);
 
 	if (ifs)
 		ifs_set_range_dirty(folio, ifs, off, len);
@@ -181,7 +182,7 @@ static void iomap_set_range_dirty(struct folio *folio, size_t off, size_t len)
 static struct iomap_folio_state *ifs_alloc(struct inode *inode,
 		struct folio *folio, unsigned int flags)
 {
-	struct iomap_folio_state *ifs = folio->private;
+	struct iomap_folio_state *ifs = folio_get_private(folio);
 	unsigned int nr_blocks = i_blocks_per_folio(inode, folio);
 	gfp_t gfp;
 
@@ -233,7 +234,7 @@ static void ifs_free(struct folio *folio)
 static void iomap_adjust_read_range(struct inode *inode, struct folio *folio,
 		loff_t *pos, loff_t length, size_t *offp, size_t *lenp)
 {
-	struct iomap_folio_state *ifs = folio->private;
+	struct iomap_folio_state *ifs = folio_get_private(folio);
 	loff_t orig_pos = *pos;
 	loff_t isize = i_size_read(inode);
 	unsigned block_bits = inode->i_blkbits;
@@ -291,7 +292,7 @@ static void iomap_adjust_read_range(struct inode *inode, struct folio *folio,
 static void iomap_finish_folio_read(struct folio *folio, size_t off,
 		size_t len, int error)
 {
-	struct iomap_folio_state *ifs = folio->private;
+	struct iomap_folio_state *ifs = folio_get_private(folio);
 	bool uptodate = !error;
 	bool finished = true;
 
@@ -567,7 +568,7 @@ EXPORT_SYMBOL_GPL(iomap_readahead);
  */
 bool iomap_is_partially_uptodate(struct folio *folio, size_t from, size_t count)
 {
-	struct iomap_folio_state *ifs = folio->private;
+	struct iomap_folio_state *ifs = folio_get_private(folio);
 	struct inode *inode = folio->mapping->host;
 	unsigned first, last, i;
 
@@ -1061,7 +1062,7 @@ static void iomap_write_delalloc_ifs_punch(struct inode *inode,
 	 * but not dirty. In that case it is necessary to punch
 	 * out such blocks to avoid leaking any delalloc blocks.
 	 */
-	ifs = folio->private;
+	ifs = folio_get_private(folio);
 	if (!ifs)
 		return;
 
@@ -1520,7 +1521,7 @@ EXPORT_SYMBOL_GPL(iomap_page_mkwrite);
 static void iomap_finish_folio_write(struct inode *inode, struct folio *folio,
 		size_t len)
 {
-	struct iomap_folio_state *ifs = folio->private;
+	struct iomap_folio_state *ifs = folio_get_private(folio);
 
 	WARN_ON_ONCE(i_blocks_per_folio(inode, folio) > 1 && !ifs);
 	WARN_ON_ONCE(ifs && atomic_read(&ifs->write_bytes_pending) <= 0);
@@ -1767,7 +1768,7 @@ static int iomap_add_to_ioend(struct iomap_writepage_ctx *wpc,
 		struct inode *inode, loff_t pos, loff_t end_pos,
 		unsigned len)
 {
-	struct iomap_folio_state *ifs = folio->private;
+	struct iomap_folio_state *ifs = folio_get_private(folio);
 	size_t poff = offset_in_folio(folio, pos);
 	int error;
 
@@ -1851,7 +1852,7 @@ static int iomap_writepage_map_blocks(struct iomap_writepage_ctx *wpc,
 
 		map_len = min_t(u64, dirty_len,
 			wpc->iomap.offset + wpc->iomap.length - pos);
-		WARN_ON_ONCE(!folio->private && map_len < dirty_len);
+		WARN_ON_ONCE(!folio_get_private(folio) && map_len < dirty_len);
 
 		switch (wpc->iomap.type) {
 		case IOMAP_INLINE:
@@ -1946,7 +1947,7 @@ static bool iomap_writepage_handle_eof(struct folio *folio, struct inode *inode,
 static int iomap_writepage_map(struct iomap_writepage_ctx *wpc,
 		struct writeback_control *wbc, struct folio *folio)
 {
-	struct iomap_folio_state *ifs = folio->private;
+	struct iomap_folio_state *ifs = folio_get_private(folio);
 	struct inode *inode = folio->mapping->host;
 	u64 pos = folio_pos(folio);
 	u64 end_pos = pos + folio_size(folio);
diff --git a/fs/jfs/jfs_metapage.c b/fs/jfs/jfs_metapage.c
index df575a873ec6..9f1f3b69d4b8 100644
--- a/fs/jfs/jfs_metapage.c
+++ b/fs/jfs/jfs_metapage.c
@@ -82,7 +82,7 @@ struct meta_anchor {
 
 static inline struct metapage *folio_to_mp(struct folio *folio, int offset)
 {
-	struct meta_anchor *anchor = folio->private;
+	struct meta_anchor *anchor = folio_get_private(folio);
 
 	if (!anchor)
 		return NULL;
@@ -95,7 +95,7 @@ static inline int insert_metapage(struct folio *folio, struct metapage *mp)
 	int index;
 	int l2mp_blocks;	/* log2 blocks per metapage */
 
-	a = folio->private;
+	a = folio_get_private(folio);
 	if (!a) {
 		a = kzalloc(sizeof(struct meta_anchor), GFP_NOFS);
 		if (!a)
@@ -116,7 +116,7 @@ static inline int insert_metapage(struct folio *folio, struct metapage *mp)
 
 static inline void remove_metapage(struct folio *folio, struct metapage *mp)
 {
-	struct meta_anchor *a = folio->private;
+	struct meta_anchor *a = folio_get_private(folio);
 	int l2mp_blocks = L2PSIZE - folio->mapping->host->i_blkbits;
 	int index;
 
@@ -134,7 +134,7 @@ static inline void remove_metapage(struct folio *folio, struct metapage *mp)
 
 static inline void inc_io(struct folio *folio)
 {
-	struct meta_anchor *anchor = folio->private;
+	struct meta_anchor *anchor = folio_get_private(folio);
 
 	atomic_inc(&anchor->io_count);
 }
@@ -142,7 +142,7 @@ static inline void inc_io(struct folio *folio)
 static inline void dec_io(struct folio *folio, blk_status_t status,
 		void (*handler)(struct folio *, blk_status_t))
 {
-	struct meta_anchor *anchor = folio->private;
+	struct meta_anchor *anchor = folio_get_private(folio);
 
 	if (anchor->status == BLK_STS_OK)
 		anchor->status = status;
@@ -154,7 +154,7 @@ static inline void dec_io(struct folio *folio, blk_status_t status,
 #else
 static inline struct metapage *folio_to_mp(struct folio *folio, int offset)
 {
-	return folio->private;
+	return folio_get_private(folio);
 }
 
 static inline int insert_metapage(struct folio *folio, struct metapage *mp)
@@ -332,7 +332,7 @@ static void metapage_write_end_io(struct bio *bio)
 {
 	struct folio *folio = bio->bi_private;
 
-	BUG_ON(!folio->private);
+	BUG_ON(!folio_get_private(folio));
 
 	dec_io(folio, bio->bi_status, last_write_complete);
 	bio_put(bio);
@@ -498,7 +498,7 @@ static int metapage_read_folio(struct file *fp, struct folio *folio)
 		pblock = metapage_get_blocks(inode, page_start + block_offset,
 					     &xlen);
 		if (pblock) {
-			if (!folio->private)
+			if (!folio_get_private(folio))
 				insert_metapage(folio, NULL);
 			inc_io(folio);
 			if (bio)
diff --git a/fs/nfs/write.c b/fs/nfs/write.c
index 82ae2b85d393..38910b1693e2 100644
--- a/fs/nfs/write.c
+++ b/fs/nfs/write.c
@@ -7,6 +7,7 @@
  * Copyright (C) 1996, 1997, Olaf Kirch <okir@monad.swb.de>
  */
 
+#include "linux/duplication.h"
 #include <linux/types.h>
 #include <linux/slab.h>
 #include <linux/mm.h>
@@ -185,7 +186,7 @@ static struct nfs_page *nfs_folio_find_head_request(struct folio *folio)
 	if (!folio_test_private(folio))
 		return NULL;
 	spin_lock(&mapping->i_private_lock);
-	req = folio->private;
+	req = folio_get_private(folio);
 	if (req) {
 		WARN_ON_ONCE(req->wb_head != req);
 		kref_get(&req->wb_kref);
@@ -584,7 +585,7 @@ static struct nfs_page *nfs_lock_and_join_requests(struct folio *folio)
 	}
 
 	/* Ensure that nobody removed the request before we locked it */
-	if (head != folio->private) {
+	if (head != folio_get_private(folio)) {
 		nfs_unlock_and_release_request(head);
 		goto retry;
 	}
diff --git a/fs/orangefs/inode.c b/fs/orangefs/inode.c
index aae6d2b8767d..904be59798c5 100644
--- a/fs/orangefs/inode.c
+++ b/fs/orangefs/inode.c
@@ -10,6 +10,7 @@
  *  Linux VFS inode operations.
  */
 
+#include "linux/duplication.h"
 #include <linux/blkdev.h>
 #include <linux/fileattr.h>
 #include "protocol.h"
@@ -150,7 +151,7 @@ static int orangefs_writepages_callback(struct folio *folio,
 		struct writeback_control *wbc, void *data)
 {
 	struct orangefs_writepages *ow = data;
-	struct orangefs_write_range *wr = folio->private;
+	struct orangefs_write_range *wr = folio_get_private(folio);
 	int ret;
 
 	if (!wr) {
diff --git a/fs/proc/meminfo.c b/fs/proc/meminfo.c
index 245171d9164b..496bfd280080 100644
--- a/fs/proc/meminfo.c
+++ b/fs/proc/meminfo.c
@@ -62,6 +62,7 @@ static int meminfo_proc_show(struct seq_file *m, void *v)
 	show_val_kb(m, "MemAvailable:   ", available);
 	show_val_kb(m, "Buffers:        ", i.bufferram);
 	show_val_kb(m, "Cached:         ", cached);
+	show_val_kb(m, "Twins:		", global_node_page_state(NR_FILE_TWINS));
 	show_val_kb(m, "SwapCached:     ", total_swapcache_pages());
 	show_val_kb(m, "Active:         ", pages[LRU_ACTIVE_ANON] +
 					   pages[LRU_ACTIVE_FILE]);
diff --git a/fs/ubifs/file.c b/fs/ubifs/file.c
index 5130123005e4..bac532811a95 100644
--- a/fs/ubifs/file.c
+++ b/fs/ubifs/file.c
@@ -37,6 +37,7 @@
  * set as well. However, UBIFS disables readahead.
  */
 
+#include "linux/duplication.h"
 #include "ubifs.h"
 #include <linux/mount.h>
 #include <linux/slab.h>
@@ -109,7 +110,7 @@ static int do_readpage(struct folio *folio)
 	dbg_gen("ino %lu, pg %lu, i_size %lld, flags %#lx",
 		inode->i_ino, folio->index, i_size, folio->flags);
 	ubifs_assert(c, !folio_test_checked(folio));
-	ubifs_assert(c, !folio->private);
+	ubifs_assert(c, !folio_get_private(folio));
 
 	addr = kmap_local_folio(folio, 0);
 
@@ -260,7 +261,7 @@ static int write_begin_slow(struct address_space *mapping,
 		}
 	}
 
-	if (folio->private)
+	if (folio_get_private(folio))
 		/*
 		 * The folio is dirty, which means it was budgeted twice:
 		 *   o first time the budget was allocated by the task which
@@ -321,7 +322,7 @@ static int allocate_budget(struct ubifs_info *c, struct folio *folio,
 {
 	struct ubifs_budget_req req = { .fast = 1 };
 
-	if (folio->private) {
+	if (folio_get_private(folio)) {
 		if (!appending)
 			/*
 			 * The folio is dirty and we are not appending, which
@@ -514,7 +515,7 @@ static void cancel_budget(struct ubifs_info *c, struct folio *folio,
 			ubifs_release_dirty_inode_budget(c, ui);
 		mutex_unlock(&ui->ui_mutex);
 	}
-	if (!folio->private) {
+	if (folio_get_private(folio)) {
 		if (folio_test_checked(folio))
 			release_new_page_budget(c);
 		else
@@ -561,7 +562,7 @@ static int ubifs_write_end(struct file *file, struct address_space *mapping,
 	if (len == folio_size(folio))
 		folio_mark_uptodate(folio);
 
-	if (!folio->private) {
+	if (!folio_get_private(folio)) {
 		folio_attach_private(folio, (void *)1);
 		atomic_long_inc(&c->dirty_pg_cnt);
 		filemap_dirty_folio(mapping, folio);
@@ -940,7 +941,7 @@ static int do_writepage(struct folio *folio, size_t len)
 		ubifs_ro_mode(c, err);
 	}
 
-	ubifs_assert(c, folio->private != NULL);
+	ubifs_assert(c, folio_get_private(folio) != NULL);
 	if (folio_test_checked(folio))
 		release_new_page_budget(c);
 	else
@@ -1012,7 +1013,7 @@ static int ubifs_writepage(struct folio *folio, struct writeback_control *wbc,
 
 	dbg_gen("ino %lu, pg %lu, pg flags %#lx",
 		inode->i_ino, folio->index, folio->flags);
-	ubifs_assert(c, folio->private != NULL);
+	ubifs_assert(c, folio_get_private(folio) != NULL);
 
 	/* Is the folio fully outside @i_size? (truncate in progress) */
 	if (folio_pos(folio) >= i_size) {
@@ -1166,7 +1167,7 @@ static int do_truncation(struct ubifs_info *c, struct inode *inode,
 				 * 'ubifs_jnl_truncate()' will see an already
 				 * truncated (and up to date) data node.
 				 */
-				ubifs_assert(c, folio->private != NULL);
+				ubifs_assert(c, folio_get_private(folio) != NULL);
 
 				folio_clear_dirty_for_io(folio);
 				if (UBIFS_BLOCKS_PER_PAGE_SHIFT)
@@ -1565,7 +1566,7 @@ static vm_fault_t ubifs_vm_page_mkwrite(struct vm_fault *vmf)
 		goto sigbus;
 	}
 
-	if (folio->private)
+	if (folio_get_private(folio))
 		release_new_page_budget(c);
 	else {
 		if (!folio_test_checked(folio))
diff --git a/include/linux/duplication.h b/include/linux/duplication.h
new file mode 100644
index 000000000000..58cf2a938367
--- /dev/null
+++ b/include/linux/duplication.h
@@ -0,0 +1,344 @@
+#ifndef _LINUX_DUPLICATION_MM_H
+#define _LINUX_DUPLICATION_MM_H
+
+#include <linux/kernel.h>
+#include "linux/kref.h"
+#include "linux/page-flags.h"
+#include "linux/page_ref.h"
+#include "linux/printk.h"
+#include <linux/workqueue.h>
+#include "linux/kref.h"
+#include <linux/mm.h>
+#include <linux/highmem.h>
+#include <linux/bitmap.h>
+
+#define copy_struct_page(to, from) memcpy((to), (from), sizeof(struct page))
+#define copy_struct_folio(to, from) memcpy((to), (from), sizeof(struct folio))
+
+extern bool duplication_switch_main_eviction;
+
+#define DUPLICATION_THRESHOLD 0
+#define DUPLICATION_DEBUG
+
+struct duplication_work_struct {
+	struct folio *src;
+	struct work_struct work;
+	int numa_node;
+};
+
+#define duplication_for_each_twin(duplication, f, n) \
+	for ((n) = 0; (n) < nr_online_nodes; (n)++)  \
+		if ((n) != (duplication)->main &&    \
+		    ((f) = find_twin((duplication), n)))
+
+struct duplication_stats {
+	unsigned long nr_struct_duplication;
+	unsigned long mains;
+	unsigned long twins;
+	unsigned long truncates;
+	unsigned long migrations_mains;
+	unsigned long migrations_twins;
+	unsigned long remove_mapping_mains;
+	unsigned long remove_mapping_twins;
+	unsigned long switch_main;
+	unsigned long switch_main_write;
+	unsigned long local_read;
+	unsigned long local_write;
+	unsigned long distant_read;
+	unsigned long distant_write;
+};
+
+DECLARE_PER_CPU(struct duplication_stats, duplication_stats);
+
+struct duplication {
+	void *private;
+	// at most 8 numa nodes
+	// stored on a single 64 bits long
+	uint8_t counter[8];
+	int main;
+	struct kref ref;
+	spinlock_t lock;
+	struct folio *folios[];
+};
+
+static inline struct folio *
+duplication_get_main(struct duplication *duplication)
+{
+	if (duplication->main == -1)
+		return NULL;
+	return duplication->folios[duplication->main];
+}
+
+static inline void *folio_get_private(struct folio *folio)
+{
+	if (folio_test_main(folio))
+		return folio->duplication->private;
+	// twin folios don't have private data
+	if (folio_test_twin(folio))
+		return NULL;
+
+	return folio->private;
+}
+
+/**
+ * folio_attach_private - Attach private data to a folio.
+ * @folio: Folio to attach data to.
+ * @data: Data to attach to folio.
+ *
+ * Attaching private data to a folio increments the page's reference count.
+ * The data must be detached before the folio will be freed.
+ */
+static inline void folio_attach_private(struct folio *folio, void *data)
+{
+	folio_get(folio);
+	if (folio_test_twin(folio) || folio_test_main(folio)) {
+		folio->duplication->private = data;
+	} else {
+		folio->private = data;
+	}
+	folio_set_private(folio);
+}
+
+/**
+ * folio_change_private - Change private data on a folio.
+ * @folio: Folio to change the data on.
+ * @data: Data to set on the folio.
+ *
+ * Change the private data attached to a folio and return the old
+ * data.  The page must previously have had data attached and the data
+ * must be detached before the folio will be freed.
+ *
+ * Return: Data that was previously attached to the folio.
+ */
+static inline void *folio_change_private(struct folio *folio, void *data)
+{
+	void *old;
+
+	if (folio_test_twin(folio) || folio_test_main(folio)) {
+		old = folio->duplication->private;
+		folio->duplication->private = data;
+	} else {
+		old = folio->private;
+		folio->private = data;
+	}
+	return old;
+}
+
+/**
+ * folio_detach_private - Detach private data from a folio.
+ * @folio: Folio to detach data from.
+ *
+ * Removes the data that was previously attached to the folio and decrements
+ * the refcount on the page.
+ *
+ * Return: Data that was attached to the folio.
+ */
+static inline void *folio_detach_private(struct folio *folio)
+{
+	void *data = folio_get_private(folio);
+
+	if (!folio_test_private(folio))
+		return NULL;
+	folio_clear_private(folio);
+	if (folio_test_twin(folio) || folio_test_main(folio))
+		folio->duplication->private = NULL;
+	else
+		folio->private = NULL;
+
+	folio_put(folio);
+
+	return data;
+}
+
+extern bool duplication_async;
+extern bool duplication_enable;
+
+// functions used in the kernel
+struct folio *duplication_find_or_duplicate(struct folio *folio,
+					    int current_nid);
+void duplication_truncate_hook(struct folio *folio);
+struct folio *duplication_folio_for_writing(struct folio *folio);
+void duplication_migration_hook(struct folio *from, struct folio *to);
+void duplication_eviction(struct folio *folio, bool truncate);
+void duplication_twin_eviction(struct folio *folio);
+
+void duplication_release_folio(struct folio *folio, struct address_space *as);
+
+void async_folio_duplication(struct work_struct *work);
+
+bool duplication_start_async_work(struct folio *folio, int nid);
+struct folio *folio_duplicate_to_node(struct folio *src, int nid,
+				      struct duplication *duplication);
+
+struct folio *duplication_set_twin_uptodate(struct folio *folio,
+					    struct duplication *duplication);
+struct folio *folio_try_duplicate(struct folio *folio, int nid,
+				  struct duplication *duplication);
+
+void duplication_debugfs(void);
+
+struct duplication *retrieve_struct_duplication(struct folio *folio);
+
+static inline void quick_print_folio(struct folio *f, bool force, char *str)
+{
+	int rf = folio_ref_count(f);
+	int mc = folio_mapcount(f);
+
+	if (!force && rf > 1)
+		return;
+	pr_err("%s: %pGp (mp %d) (rf %d) %ld) 0x%p [folio 0x%lx] idx %ld\n",
+	       str, &f->flags, mc, rf, folio_nr_pages(f), f->mapping,
+	       folio_pfn(f), f->index);
+}
+
+static inline void kill_current(void)
+{
+	dump_stack();
+	do_send_sig_info(SIGKILL, SEND_SIG_PRIV, current, PIDTYPE_TGID);
+}
+
+static inline void kill_current_on(bool cond)
+{
+	if (cond)
+		kill_current();
+}
+
+static inline void
+quick_print_struct_duplication(struct duplication *duplication, bool bug,
+			       char *str)
+{
+	struct folio *folio;
+
+	pr_err("quick print duplication %s:\n", str);
+	folio = duplication_get_main(duplication);
+
+	if (folio)
+		quick_print_folio(folio, true, "main");
+
+	for (int n = 0; n < nr_online_nodes; n++) {
+		folio = duplication->folios[n];
+		if (folio)
+			quick_print_folio(folio, true, "duplicate");
+	}
+	BUG_ON(bug);
+}
+
+static inline struct folio *find_twin(struct duplication *duplication, int nid)
+{
+	struct folio *folio = NULL;
+
+	// BUG_ON(!folio_test_locked(duplication->main));
+
+	return folio = duplication->folios[nid];
+}
+
+void struct_duplication_release(struct kref *kref);
+
+static inline void struct_duplication_put(struct duplication *duplication)
+{
+	kref_put(&duplication->ref, struct_duplication_release);
+}
+
+static inline void struct_duplication_get(struct duplication *duplication)
+{
+	kref_get(&duplication->ref);
+}
+
+// lock on struct must be taken
+static inline void
+duplication_increment_counter(int nid, struct duplication *duplication)
+{
+	duplication->counter[nid]++;
+}
+
+static inline void
+duplication_decrement_counter(int nid, struct duplication *duplication)
+{
+	duplication->counter[nid]--;
+}
+
+static inline void duplication_reset_counter(int nid,
+					     struct duplication *duplication)
+{
+	duplication->counter[nid] = 0;
+}
+
+static inline void duplication_reset_counters(struct duplication *duplication)
+{
+	for (int i = 0; i < nr_online_nodes; i++)
+		duplication->counter[i] = 0;
+}
+
+static inline uint8_t duplication_get_counter(int nid,
+					      struct duplication *duplication)
+{
+	return duplication->counter[nid];
+}
+
+static inline void struct_add_twin(struct folio *folio,
+				   struct duplication *duplication)
+{
+	int nr = folio_nr_pages(folio);
+	struct_duplication_get(duplication);
+	folio->duplication = duplication;
+	this_cpu_inc(duplication_stats.twins);
+	__folio_mark_uptodate(folio);
+	__lruvec_stat_mod_folio(folio, NR_FILE_PAGES, nr);
+	__lruvec_stat_mod_folio(folio, NR_FILE_TWINS, nr);
+	if (folio_test_pmd_mappable(folio))
+		__lruvec_stat_mod_folio(folio, NR_FILE_THPS, nr);
+	folio_set_twin(folio);
+	duplication->folios[folio_nid(folio)] = folio;
+}
+
+static inline void struct_remove_twin(struct folio *folio,
+				      struct duplication *duplication)
+{
+	int nr = folio_nr_pages(folio);
+	WARN_ONCE(!folio_test_twin(folio), "attempt to remove a non twin");
+	folio->private = NULL;
+	folio_clear_oldmain(folio);
+	folio_set_oldtwin(folio);
+	folio_clear_twin(folio);
+	this_cpu_dec(duplication_stats.twins);
+	__folio_mark_uptodate(folio);
+	duplication->folios[folio_nid(folio)] = NULL;
+	__lruvec_stat_mod_folio(folio, NR_FILE_PAGES, -nr);
+	__lruvec_stat_mod_folio(folio, NR_FILE_TWINS, -nr);
+	if (folio_test_pmd_mappable(folio))
+		__lruvec_stat_mod_folio(folio, NR_FILE_THPS, -nr);
+	duplication_reset_counter(folio_nid(folio), duplication);
+	struct_duplication_put(duplication);
+}
+
+static inline void struct_remove_main(struct folio *folio,
+				      struct duplication *duplication)
+{
+	WARN_ONCE(!folio_test_main(folio), "non main in remove_main");
+	WARN_ONCE(!folio_test_locked(folio), "nonlocked during remove twin");
+	folio->private = duplication->private;
+	duplication->main = -1;
+	duplication->folios[folio_nid(folio)] = NULL;
+	this_cpu_dec(duplication_stats.mains);
+	folio_clear_oldtwin(folio);
+	folio_clear_main(folio);
+	folio_set_oldmain(folio);
+	struct_duplication_put(duplication);
+}
+
+static inline void struct_add_main(struct folio *folio,
+				   struct duplication *duplication)
+{
+	int nid = folio_nid(folio);
+	BUG_ON(duplication->main != -1);
+
+	duplication->private = folio->private;
+	folio_set_main(folio);
+	this_cpu_inc(duplication_stats.mains);
+	folio->duplication = duplication;
+	struct_duplication_get(duplication);
+	duplication->main = nid;
+	duplication->folios[nid] = folio;
+}
+
+#endif
diff --git a/include/linux/fscrypt.h b/include/linux/fscrypt.h
index 772f822dc6b8..8e5bf807597d 100644
--- a/include/linux/fscrypt.h
+++ b/include/linux/fscrypt.h
@@ -17,6 +17,7 @@
 #include <linux/mm.h>
 #include <linux/slab.h>
 #include <uapi/linux/fscrypt.h>
+#include <linux/duplication.h>
 
 /*
  * The lengths of all file contents blocks must be divisible by this value.
@@ -340,7 +341,7 @@ static inline bool fscrypt_is_bounce_folio(struct folio *folio)
 
 static inline struct folio *fscrypt_pagecache_folio(struct folio *bounce_folio)
 {
-	return bounce_folio->private;
+	return folio_get_private(bounce_folio);
 }
 
 void fscrypt_free_bounce_page(struct page *bounce_page);
diff --git a/include/linux/migrate.h b/include/linux/migrate.h
index 002e49b2ebd9..2734f64e7574 100644
--- a/include/linux/migrate.h
+++ b/include/linux/migrate.h
@@ -233,6 +233,7 @@ void migrate_device_pages(unsigned long *src_pfns, unsigned long *dst_pfns,
 			unsigned long npages);
 void migrate_device_finalize(unsigned long *src_pfns,
 			unsigned long *dst_pfns, unsigned long npages);
+bool duplication_try_switch_main(struct folio *folio, int nid);
 
 #endif /* CONFIG_MIGRATION */
 
diff --git a/include/linux/mm.h b/include/linux/mm.h
index 8617adc6becd..5e2dad4ad4bd 100644
--- a/include/linux/mm.h
+++ b/include/linux/mm.h
@@ -2,6 +2,7 @@
 #ifndef _LINUX_MM_H
 #define _LINUX_MM_H
 
+#include "linux/printk.h"
 #include <linux/errno.h>
 #include <linux/mmdebug.h>
 #include <linux/gfp.h>
diff --git a/include/linux/mm_types.h b/include/linux/mm_types.h
index 6894de506b36..587fa7570be1 100644
--- a/include/linux/mm_types.h
+++ b/include/linux/mm_types.h
@@ -31,6 +31,7 @@
 
 struct address_space;
 struct mem_cgroup;
+struct duplication;
 
 /*
  * Each physical page in the system has a struct page associated with
@@ -112,7 +113,10 @@ struct page {
 			 * Used for swp_entry_t if swapcache flag set.
 			 * Indicates order in the buddy system if PageBuddy.
 			 */
-			unsigned long private;
+			union {
+				unsigned long private;
+				struct duplication *duplication;
+			};
 		};
 		struct {	/* page_pool used by netstack */
 			/**
@@ -343,6 +347,7 @@ struct folio {
 			union {
 				void *private;
 				swp_entry_t swap;
+				struct duplication *duplication;
 			};
 			atomic_t _mapcount;
 			atomic_t _refcount;
@@ -567,11 +572,6 @@ static inline void set_page_private(struct page *page, unsigned long private)
 	page->private = private;
 }
 
-static inline void *folio_get_private(struct folio *folio)
-{
-	return folio->private;
-}
-
 struct page_frag_cache {
 	void * va;
 #if (PAGE_SIZE < PAGE_FRAG_CACHE_MAX_SIZE)
diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 80bc5640bb60..9acf21f4eaa5 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -183,6 +183,7 @@ enum node_stat_item {
 	NR_FILE_MAPPED,	/* pagecache pages mapped into pagetables.
 			   only modified from process context */
 	NR_FILE_PAGES,
+	NR_FILE_TWINS,
 	NR_FILE_DIRTY,
 	NR_WRITEBACK,
 	NR_WRITEBACK_TEMP,	/* Writeback using temporary buffers */
diff --git a/include/linux/page-flags.h b/include/linux/page-flags.h
index 48c66b846682..d2e0a82947fd 100644
--- a/include/linux/page-flags.h
+++ b/include/linux/page-flags.h
@@ -126,6 +126,10 @@ enum pageflags {
 #ifdef CONFIG_ARCH_USES_PG_ARCH_3
 	PG_arch_3,
 #endif
+	PG_main, /* A duplicate struct is associated with the page */
+	PG_twin, /* A duplicate struct is associated with the page, this is a twin */
+	PG_oldmain, 
+	PG_oldtwin,
 	__NR_PAGEFLAGS,
 
 	PG_readahead = PG_reclaim,
@@ -611,6 +615,11 @@ PAGEFLAG_FALSE(HWPoison, hwpoison)
 #define __PG_HWPOISON 0
 #endif
 
+FOLIO_FLAG(main, FOLIO_HEAD_PAGE)
+FOLIO_FLAG(twin, FOLIO_HEAD_PAGE)
+FOLIO_FLAG(oldmain, FOLIO_HEAD_PAGE)
+FOLIO_FLAG(oldtwin, FOLIO_HEAD_PAGE)
+
 #ifdef CONFIG_PAGE_IDLE_FLAG
 #ifdef CONFIG_64BIT
 FOLIO_TEST_FLAG(young, FOLIO_HEAD_PAGE)
@@ -1163,7 +1172,8 @@ static __always_inline void __ClearPageAnonExclusive(struct page *page)
 	 1UL << PG_private	| 1UL << PG_private_2	|	\
 	 1UL << PG_writeback	| 1UL << PG_reserved	|	\
 	 1UL << PG_active 	|				\
-	 1UL << PG_unevictable	| __PG_MLOCKED | LRU_GEN_MASK)
+	 1UL << PG_unevictable	| __PG_MLOCKED | LRU_GEN_MASK  |\
+	 1UL << PG_twin | 1UL << PG_main)
 
 /*
  * Flags checked when a page is prepped for return by the page allocator.
diff --git a/include/linux/pagemap.h b/include/linux/pagemap.h
index 68a5f1ff3301..a618270e1bf4 100644
--- a/include/linux/pagemap.h
+++ b/include/linux/pagemap.h
@@ -15,6 +15,7 @@
 #include <linux/bitops.h>
 #include <linux/hardirq.h> /* for in_interrupt() */
 #include <linux/hugetlb_inline.h>
+#include <linux/duplication.h>
 
 struct folio_batch;
 
@@ -594,62 +595,6 @@ static inline struct inode *folio_inode(struct folio *folio)
 	return folio->mapping->host;
 }
 
-/**
- * folio_attach_private - Attach private data to a folio.
- * @folio: Folio to attach data to.
- * @data: Data to attach to folio.
- *
- * Attaching private data to a folio increments the page's reference count.
- * The data must be detached before the folio will be freed.
- */
-static inline void folio_attach_private(struct folio *folio, void *data)
-{
-	folio_get(folio);
-	folio->private = data;
-	folio_set_private(folio);
-}
-
-/**
- * folio_change_private - Change private data on a folio.
- * @folio: Folio to change the data on.
- * @data: Data to set on the folio.
- *
- * Change the private data attached to a folio and return the old
- * data.  The page must previously have had data attached and the data
- * must be detached before the folio will be freed.
- *
- * Return: Data that was previously attached to the folio.
- */
-static inline void *folio_change_private(struct folio *folio, void *data)
-{
-	void *old = folio_get_private(folio);
-
-	folio->private = data;
-	return old;
-}
-
-/**
- * folio_detach_private - Detach private data from a folio.
- * @folio: Folio to detach data from.
- *
- * Removes the data that was previously attached to the folio and decrements
- * the refcount on the page.
- *
- * Return: Data that was attached to the folio.
- */
-static inline void *folio_detach_private(struct folio *folio)
-{
-	void *data = folio_get_private(folio);
-
-	if (!folio_test_private(folio))
-		return NULL;
-	folio_clear_private(folio);
-	folio->private = NULL;
-	folio_put(folio);
-
-	return data;
-}
-
 static inline void attach_page_private(struct page *page, void *data)
 {
 	folio_attach_private(page_folio(page), data);
@@ -1210,7 +1155,7 @@ static inline vm_fault_t folio_lock_or_retry(struct folio *folio,
 void folio_wait_bit(struct folio *folio, int bit_nr);
 int folio_wait_bit_killable(struct folio *folio, int bit_nr);
 
-/* 
+/*
  * Wait for a folio to be unlocked.
  *
  * This must be called with the caller "holding" the folio,
@@ -1429,6 +1374,11 @@ static inline struct folio *__readahead_folio(struct readahead_control *ractl)
 		return NULL;
 	}
 
+	struct address_space *mapping = ractl->mapping;
+	BUG_ON(!mapping);
+	struct xarray *xarray = &mapping->i_pages;
+	BUG_ON(!xarray);
+	BUG_ON(!ractl);
 	folio = xa_load(&ractl->mapping->i_pages, ractl->_index);
 	VM_BUG_ON_FOLIO(!folio_test_locked(folio), folio);
 	ractl->_batch_count = folio_nr_pages(folio);
diff --git a/include/linux/rmap.h b/include/linux/rmap.h
index d5e93e44322e..afb7fd3dea7b 100644
--- a/include/linux/rmap.h
+++ b/include/linux/rmap.h
@@ -203,6 +203,8 @@ static inline void __folio_rmap_sanity_checks(struct folio *folio,
 	/* When (un)mapping zeropages, we should never touch ref+mapcount. */
 	VM_WARN_ON_FOLIO(is_zero_folio(folio), folio);
 
+	VM_WARN_ON_FOLIO(folio_test_twin(folio), folio);
+	VM_WARN_ON_FOLIO(folio_test_main(folio), folio);
 	/*
 	 * TODO: we get driver-allocated folios that have nothing to do with
 	 * the rmap using vm_insert_page(); therefore, we cannot assume that
diff --git a/include/trace/events/duplication.h b/include/trace/events/duplication.h
new file mode 100644
index 000000000000..fe0c7c822b15
--- /dev/null
+++ b/include/trace/events/duplication.h
@@ -0,0 +1,186 @@
+#undef TRACE_SYSTEM
+#define TRACE_SYSTEM duplication
+
+#if !defined(_DUPLICATION_TRACE_H) || defined(TRACE_HEADER_MULTI_READ)
+#define _DUPLICATION_TRACE_H
+
+#include <linux/tracepoint.h>
+#include <trace/events/mmflags.h>
+
+DECLARE_EVENT_CLASS(duplication_op_dual,
+
+	TP_PROTO(struct folio *main, struct folio *dup),
+
+	TP_ARGS(main, dup),
+
+	TP_STRUCT__entry(
+		__field(	unsigned long,	main_pfn	)
+		__field(	unsigned long,	dup_pfn	)
+		__field(	unsigned long,	main_flags	)
+		__field(	unsigned long,	dup_flags	)
+		__field(	int,	order	)
+	),
+
+	TP_fast_assign(
+		__entry->main_pfn = folio_pfn(main);
+		__entry->dup_pfn = folio_pfn(dup);
+		__entry->main_flags = main->flags;
+		__entry->dup_flags = dup->flags;
+		__entry->order = folio_order(dup);
+	),
+
+	TP_printk("main: 0x%lx duplicate: 0x%lx (%s %s) order: %d", __entry->main_pfn, __entry->dup_pfn, show_page_flags(__entry->main_flags), show_page_flags(__entry->dup_flags), __entry->order)
+);
+
+DECLARE_EVENT_CLASS(duplication_op_single,
+
+	TP_PROTO(struct folio *folio),
+
+	TP_ARGS(folio),
+
+	TP_STRUCT__entry(
+		__field(	unsigned long,	pfn	)
+		__field(	unsigned long*,	flags	)
+		__field(	int,	order	)
+	),
+
+	TP_fast_assign(
+		__entry->pfn = folio_pfn(folio);
+		__entry->flags = &folio->flags;
+		__entry->order = folio_order(folio);
+	),
+
+	TP_printk("folio: 0x%lx (%pGp) order: %d", __entry->pfn, __entry->flags, __entry->order)
+);
+
+DECLARE_EVENT_CLASS(duplication_op_struct_duplication,
+
+	TP_PROTO(struct duplication *duplication),
+
+	TP_ARGS(duplication),
+
+	TP_STRUCT__entry(
+		__field(	void *,	ptr	)
+	),
+
+	TP_fast_assign(
+		   __entry->ptr = duplication;
+	),
+
+	TP_printk("key: %p", __entry->ptr)
+);
+
+
+DEFINE_EVENT(duplication_op_dual, duplication_success,
+	TP_PROTO(struct folio *old, struct folio *new),
+	TP_ARGS(old, new)
+	);
+
+DEFINE_EVENT(duplication_op_dual, duplication_migration,
+	TP_PROTO(struct folio *old, struct folio *new),
+	TP_ARGS(old, new)
+	);
+
+DEFINE_EVENT(duplication_op_dual, duplication_switch_main,
+	TP_PROTO(struct folio *old, struct folio *new),
+	TP_ARGS(old, new)
+	);
+
+DEFINE_EVENT(duplication_op_dual, duplication_elect_new_main,
+	TP_PROTO(struct folio *old, struct folio *new),
+	TP_ARGS(old, new)
+	);
+
+DEFINE_EVENT(duplication_op_dual, duplication_duplicate_found,
+	TP_PROTO(struct folio *old, struct folio *new),
+	TP_ARGS(old, new)
+	);
+
+DEFINE_EVENT(duplication_op_single, duplication_local_node,
+	TP_PROTO(struct folio *folio),
+	TP_ARGS(folio)
+	);
+
+DEFINE_EVENT(duplication_op_single, duplication_try_duplicate,
+	TP_PROTO(struct folio *folio),
+	TP_ARGS(folio)
+	);
+
+DEFINE_EVENT(duplication_op_single, duplication_cannot_duplicate,
+	TP_PROTO(struct folio *folio),
+	TP_ARGS(folio)
+	);
+
+DEFINE_EVENT(duplication_op_single, duplication_duplicate_out_of_date,
+	TP_PROTO(struct folio *folio),
+	TP_ARGS(folio)
+	);
+
+DEFINE_EVENT(duplication_op_single, duplication_free_main,
+	TP_PROTO(struct folio *folio),
+	TP_ARGS(folio)
+	);
+
+DEFINE_EVENT(duplication_op_single, duplication_free_duplicated,
+	TP_PROTO(struct folio *folio),
+	TP_ARGS(folio)
+	);
+
+DEFINE_EVENT(duplication_op_single, duplication_folio_alloc_node_failed,
+	TP_PROTO(struct folio *folio),
+	TP_ARGS(folio)
+	);
+
+DEFINE_EVENT(duplication_op_single, duplication_invalidate_duplicated,
+	TP_PROTO(struct folio *folio),
+	TP_ARGS(folio)
+	);
+
+DEFINE_EVENT(duplication_op_single, duplication_write_folio,
+	TP_PROTO(struct folio *folio),
+	TP_ARGS(folio)
+	);
+
+DEFINE_EVENT(duplication_op_single, duplication_write_folio_no_struct,
+	TP_PROTO(struct folio *folio),
+	TP_ARGS(folio)
+	);
+
+DEFINE_EVENT(duplication_op_single, duplication_update_sibling,
+	TP_PROTO(struct folio *folio),
+	TP_ARGS(folio)
+	);
+
+DEFINE_EVENT(duplication_op_struct_duplication, duplication_free_struct_duplication,
+	TP_PROTO(struct duplication *duplication),
+	TP_ARGS(duplication)
+	 );
+
+DEFINE_EVENT(duplication_op_single, duplication_failed_allocation_struct_duplication,
+	TP_PROTO(struct folio *folio),
+	TP_ARGS(folio)
+	);
+
+DEFINE_EVENT(duplication_op_single, duplication_duplicate_is_being_freed,
+	TP_PROTO(struct folio *folio),
+	TP_ARGS(folio)
+	);
+
+DEFINE_EVENT(duplication_op_single, duplication_duplicate_is_not_lru,
+	TP_PROTO(struct folio *folio),
+	TP_ARGS(folio)
+	);
+
+DEFINE_EVENT(duplication_op_single, duplication_remove_mapping_hook,
+	TP_PROTO(struct folio *folio),
+	TP_ARGS(folio)
+	);
+
+DEFINE_EVENT(duplication_op_single, duplication_truncate_hook,
+	TP_PROTO(struct folio *folio),
+	TP_ARGS(folio)
+	);
+#endif /* _DUPLICATION_TRACE_H */
+
+/* This part must be outside protection */
+#include <trace/define_trace.h>
diff --git a/include/trace/events/mmflags.h b/include/trace/events/mmflags.h
index d36c857dd249..e685a4681bf0 100644
--- a/include/trace/events/mmflags.h
+++ b/include/trace/events/mmflags.h
@@ -179,7 +179,11 @@ TRACE_DEFINE_ENUM(___GFP_LAST_BIT);
 	DEF_PAGEFLAG_NAME(head),					\
 	DEF_PAGEFLAG_NAME(reclaim),					\
 	DEF_PAGEFLAG_NAME(swapbacked),					\
-	DEF_PAGEFLAG_NAME(unevictable)					\
+	DEF_PAGEFLAG_NAME(unevictable),					\
+	DEF_PAGEFLAG_NAME(main),						\
+	DEF_PAGEFLAG_NAME(twin),						\
+	DEF_PAGEFLAG_NAME(oldmain),						\
+	DEF_PAGEFLAG_NAME(oldtwin)						\
 IF_HAVE_PG_MLOCK(mlocked)						\
 IF_HAVE_PG_HWPOISON(hwpoison)						\
 IF_HAVE_PG_IDLE(idle)							\
diff --git a/kernel/sched/fair.c b/kernel/sched/fair.c
index 58ba14ed8fbc..dcceb718b514 100644
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@ -1914,6 +1914,10 @@ bool should_numa_migrate_memory(struct task_struct *p, struct folio *folio,
 	int dst_nid = cpu_to_node(dst_cpu);
 	int last_cpupid, this_cpupid;
 
+	if (folio_test_main(folio) || folio_test_twin(folio)) {
+		pr_err("numa balancing: should not migrate main/duplicate folio\n");
+	}
+
 	/*
 	 * Cannot migrate to memoryless nodes.
 	 */
diff --git a/kernel/signal.c b/kernel/signal.c
index 2ae45e6eb6bb..1c67fe72557f 100644
--- a/kernel/signal.c
+++ b/kernel/signal.c
@@ -1303,6 +1303,7 @@ int do_send_sig_info(int sig, struct kernel_siginfo *info, struct task_struct *p
 
 	return ret;
 }
+EXPORT_SYMBOL_GPL(do_send_sig_info);
 
 enum sig_handler {
 	HANDLER_CURRENT, /* If reachable use the current handler */
@@ -4547,7 +4548,7 @@ COMPAT_SYSCALL_DEFINE4(rt_sigaction, int, sig,
 
 	ret = do_sigaction(sig, act ? &new_ka : NULL, oact ? &old_ka : NULL);
 	if (!ret && oact) {
-		ret = put_user(ptr_to_compat(old_ka.sa.sa_handler), 
+		ret = put_user(ptr_to_compat(old_ka.sa.sa_handler),
 			       &oact->sa_handler);
 		ret |= put_compat_sigset(&oact->sa_mask, &old_ka.sa.sa_mask,
 					 sizeof(oact->sa_mask));
@@ -4726,7 +4727,7 @@ SYSCALL_DEFINE2(rt_sigsuspend, sigset_t __user *, unewset, size_t, sigsetsize)
 		return -EFAULT;
 	return sigsuspend(&newset);
 }
- 
+
 #ifdef CONFIG_COMPAT
 COMPAT_SYSCALL_DEFINE2(rt_sigsuspend, compat_sigset_t __user *, unewset, compat_size_t, sigsetsize)
 {
diff --git a/mm/Makefile b/mm/Makefile
index d5639b036166..f2b17eff7c32 100644
--- a/mm/Makefile
+++ b/mm/Makefile
@@ -145,3 +145,4 @@ obj-$(CONFIG_GENERIC_IOREMAP) += ioremap.o
 obj-$(CONFIG_SHRINKER_DEBUG) += shrinker_debug.o
 obj-$(CONFIG_EXECMEM) += execmem.o
 obj-$(CONFIG_TMPFS_QUOTA) += shmem_quota.o
+obj-y += duplication/
diff --git a/mm/duplication/Makefile b/mm/duplication/Makefile
new file mode 100644
index 000000000000..71f4c7576c88
--- /dev/null
+++ b/mm/duplication/Makefile
@@ -0,0 +1 @@
+obj-y += main.o debug.o async.o free.o duplicate.o
diff --git a/mm/duplication/async.c b/mm/duplication/async.c
new file mode 100644
index 000000000000..6b7b4324ecff
--- /dev/null
+++ b/mm/duplication/async.c
@@ -0,0 +1,43 @@
+#include <linux/duplication.h>
+#include <linux/fs.h>
+
+struct kmem_cache *slab_dws;
+
+bool duplication_start_async_work(struct folio *folio, int nid)
+{
+	struct duplication_work_struct *dws;
+	int ret = false;
+
+	dws = kmalloc(sizeof(struct duplication_work_struct), GFP_KERNEL);
+
+	if (!dws)
+		return false;
+
+	dws->numa_node = nid;
+	dws->src = folio;
+	INIT_WORK(&dws->work, async_folio_duplication);
+
+	ret = schedule_work(&dws->work);
+	if (!ret) {
+		kfree(dws);
+		goto out;
+	}
+
+out:
+	return ret;
+}
+
+void async_folio_duplication(struct work_struct *work)
+{
+	struct duplication_work_struct *dws =
+		container_of(work, struct duplication_work_struct, work);
+	int nid;
+	struct folio *src;
+
+	nid = dws->numa_node;
+	src = dws->src;
+
+	folio_duplicate_to_node(src, nid, NULL);
+
+	kfree(dws);
+}
diff --git a/mm/duplication/debug.c b/mm/duplication/debug.c
new file mode 100644
index 000000000000..9663a497bb32
--- /dev/null
+++ b/mm/duplication/debug.c
@@ -0,0 +1,124 @@
+#include "linux/printk.h"
+#include <linux/duplication.h>
+#include <linux/mm_inline.h>
+#include <linux/cpuhplock.h>
+#include <linux/debugfs.h>
+#include <linux/dcache.h>
+#include <linux/memcontrol.h>
+
+static void lru_stats(struct list_head *lru, struct lruvec *lruvec, int *main, int *twin)
+{
+	struct folio *folio;
+	*main = 0;
+	*twin = 0;
+
+	list_for_each_entry(folio, lru, lru) {
+		folio_get(folio);
+		if (folio_test_main(folio) || folio_test_twin(folio)) {
+
+			if (folio_test_main(folio))
+				(*main)++;
+			else if (folio_test_twin(folio)) {
+				(*twin)++;
+			}
+		}
+		folio_put(folio);
+	}
+}
+
+static struct lruvec *get_lruvec(struct mem_cgroup *memcg, int nid)
+{
+	struct pglist_data *pgdat = NODE_DATA(nid);
+
+#ifdef CONFIG_MEMCG
+	if (memcg) {
+		struct lruvec *lruvec = &memcg->nodeinfo[nid]->lruvec;
+
+		/* see the comment in mem_cgroup_lruvec() */
+		if (!lruvec->pgdat)
+			lruvec->pgdat = pgdat;
+
+		return lruvec;
+	}
+#endif
+	VM_WARN_ON_ONCE(!mem_cgroup_disabled());
+
+	return &pgdat->__lruvec;
+}
+
+static void duplication_cleanup(void)
+{
+	static DEFINE_MUTEX(cleanup_lock);
+	struct mem_cgroup *memcg = NULL;
+	int main, twin;
+
+	cgroup_lock();
+	cpus_read_lock();
+	get_online_mems();
+	mutex_lock(&cleanup_lock);
+
+	// drain all cpu to get access to the pages
+	lru_add_drain_all();
+
+	memcg = mem_cgroup_iter(NULL, NULL, NULL);
+	do {
+		int nid;
+
+		for_each_node(nid) {
+			struct lruvec *lruvec = get_lruvec(memcg, nid);
+
+			spin_lock_irq(&lruvec->lru_lock);
+#ifdef CONFIG_LRU_GEN
+			if (lruvec->lrugen.enabled) {
+				for (int gen = 0; gen < MAX_NR_GENS; gen++) {
+					for (int zone = 0; zone < MAX_NR_ZONES; zone++)
+						lru_stats(&lruvec->lrugen.folios[gen][WORKINGSET_FILE][zone], lruvec, &main, &twin);
+				}
+				goto unlock;
+			}
+#endif
+			for (int i = LRU_INACTIVE_FILE; i < LRU_ACTIVE_FILE + 1;
+			     i++) {
+				lru_stats(&lruvec->lists[i], lruvec, &main, &twin);
+			}
+
+#ifdef CONFIG_LRU_GEN
+unlock:
+#endif
+
+			spin_unlock_irq(&lruvec->lru_lock);
+
+			pr_err("node: %d, memcg: %d, main: %d, twin: %d\n", nid, mem_cgroup_id(memcg), main, twin);
+		}
+
+	} while ((memcg = mem_cgroup_iter(NULL, memcg, NULL)));
+	// this neat loop has been stolen from vmscan.c:lru_gen_change_state
+	mutex_unlock(&cleanup_lock);
+	put_online_mems();
+	cpus_read_unlock();
+	cgroup_unlock();
+}
+
+static ssize_t read(struct file *file, char __user *buffer, size_t size, loff_t *off) {
+	return 0;
+}
+static ssize_t write(struct file *file, const char __user *buffer, size_t size, loff_t *off) {
+	if (size > 0) {
+		duplication_cleanup();
+		*off += size;
+		return size;
+	}
+	return 0;
+}
+
+struct file_operations dup_fops = {
+	.read = read,
+	.write = write,
+};
+
+void duplication_debugfs(void)
+{
+	struct dentry *duplication = debugfs_create_dir("duplication", NULL);
+	debugfs_create_file("stats", 0777, duplication, NULL, &dup_fops);
+}
+
diff --git a/mm/duplication/duplicate.c b/mm/duplication/duplicate.c
new file mode 100644
index 000000000000..3095e808a348
--- /dev/null
+++ b/mm/duplication/duplicate.c
@@ -0,0 +1,146 @@
+#include "linux/mm_types.h"
+#include "linux/stddef.h"
+#include <linux/duplication.h>
+#include <linux/mm.h>
+#include <linux/nodemask.h>
+#include <linux/page-flags.h>
+#include <linux/pagemap.h>
+#include <linux/printk.h>
+#include <linux/swap.h>
+
+#include <trace/events/duplication.h>
+
+// requires a kref
+
+/**
+ * folio_duplicate - Duplicate the contents of one folio to another node.
+ * @src: Folio to copy from.
+ * @nid: Node to copy to.
+ */
+struct folio *folio_duplicate_to_node(struct folio *src, int nid,
+				      struct duplication *duplication)
+{
+	struct page *page_new, *page_src;
+	struct folio *new = NULL;
+	unsigned int order;
+	struct address_space *mapping;
+	int ret;
+
+	mapping = src->mapping;
+	order = folio_order(src);
+	page_src = &src->page;
+
+	if (!mapping)
+		quick_print_struct_duplication(duplication, true, "no mapping!");
+
+	/* make sure we have our pages allocated before scheduling a duplication work */
+	new = __folio_alloc_node((mapping_gfp_mask(mapping) | __GFP_THISNODE |
+				  __GFP_NOLOCKDEP | __GFP_NOWARN) &
+					 ~__GFP_RECLAIM,
+				 order, nid);
+
+	if (!new || new == src) {
+		trace_duplication_folio_alloc_node_failed(src);
+		goto out;
+	}
+
+	ret = mem_cgroup_charge(new, NULL,
+				mapping_gfp_constraint(mapping, GFP_KERNEL));
+	if (ret) {
+		folio_put(new);
+		new = NULL;
+		goto out;
+	}
+
+	page_new = &new->page;
+
+#ifdef DUPICATION_DEBUG
+	if (folio_nid(new) != nid) {
+		// pr_err("folio nid is %d, should be %d\n", folio_nid(new), nid);
+		folio_put(new);
+		new = NULL;
+		BUG();
+	}
+#endif
+
+	folio_lock(new);
+	/* copy the data */
+	folio_copy(new, src);
+
+	page_new->mapping = mapping;
+	page_new->index = page_src->index;
+
+	// new ref for the newly duplicated folio
+	folio_set_twin(new);
+	__folio_mark_uptodate(new);
+
+	// the struct duplication keeps 1 ref on each twin
+	folio_get(new);
+
+	WARN_ON(folio_ref_count(new) != 2);
+
+	/* put it in the sibling list but in a locked state*/
+	struct_add_twin(new, duplication);
+
+	new->duplication = duplication;
+
+	/* new can be unlocked */
+	folio_unlock(new);
+
+	/* add the folio to the lru list */
+	folio_add_lru(new);
+
+	trace_duplication_success(src, new);
+	BUG_ON(folio_test_private(new));
+
+out:
+	return new;
+}
+
+// Increase the ref count and return the associated struct duplication
+// if we are the main, we know the key is our pfn
+// if we are not, then we need to go to the xarray to get the pfn
+// return null is no struct duplication is associated
+struct duplication *retrieve_struct_duplication(struct folio *folio)
+{
+	struct duplication *duplication = NULL;
+
+	duplication = folio->duplication;
+	if (!duplication) {
+		quick_print_folio(folio, true, "no duplication");
+		BUG();
+	}
+	if (!kref_get_unless_zero(&duplication->ref))
+		duplication = NULL;
+	return duplication;
+}
+
+// spin lock on duplication should be taken
+struct folio *duplication_set_twin_uptodate(struct folio *folio,
+					    struct duplication *duplication)
+{
+	struct folio *main = duplication_get_main(duplication);
+
+	trace_duplication_update_sibling(folio);
+	folio->index = main->index;
+	folio->mapping = main->mapping;
+	folio_copy(folio, main);
+	__folio_mark_uptodate(folio);
+
+	return folio;
+}
+
+struct folio *folio_try_duplicate(struct folio *folio, int nid,
+				  struct duplication *duplication)
+{
+	struct folio *twin = NULL;
+
+	trace_duplication_try_duplicate(folio);
+	if (!folio_test_uptodate(folio))
+		goto out;
+
+	twin = folio_duplicate_to_node(folio, nid, duplication);
+
+out:
+	return twin;
+}
diff --git a/mm/duplication/free.c b/mm/duplication/free.c
new file mode 100644
index 000000000000..0ea321ca5379
--- /dev/null
+++ b/mm/duplication/free.c
@@ -0,0 +1,163 @@
+#include "linux/stddef.h"
+#include <linux/duplication.h>
+#include <linux/container_of.h>
+#include <linux/mm_inline.h>
+#include <linux/gfp_types.h>
+#include <linux/kref.h>
+#include <linux/mm.h>
+#include <linux/mm_types.h>
+#include <linux/page-flags.h>
+#include <linux/page_ref.h>
+#include <linux/pagemap.h>
+#include <linux/pagevec.h>
+#include <linux/printk.h>
+#include <linux/slab.h>
+#include <linux/spinlock.h>
+
+#include "../internal.h"
+
+#include <trace/events/duplication.h>
+
+void struct_duplication_release(struct kref *kref)
+{
+	struct duplication *duplication =
+		container_of(kref, struct duplication, ref);
+	BUG_ON(!duplication);
+
+#ifdef DUPLICATION_DEBUG
+	struct folio *folio = duplication_get_main(duplication);
+
+	if (folio) {
+		quick_print_folio(folio, true,
+				  "main while freeing struct duplication");
+		BUG();
+	}
+	for (int n = 0; n < nr_node_ids; n++) {
+		folio = duplication->folios[n];
+		if (folio) {
+			quick_print_folio(
+				folio, true,
+				"dupl. while freeing struct duplication");
+			BUG();
+		}
+	}
+#endif
+	this_cpu_dec(duplication_stats.nr_struct_duplication);
+
+	trace_duplication_free_struct_duplication(duplication);
+	kfree(duplication);
+}
+
+void duplication_eviction(struct folio *folio, bool truncate)
+{
+	struct duplication *duplication = NULL;
+	struct folio *twin;
+	bool pg_duplicated, pg_main;
+	pgoff_t index;
+	unsigned long flags;
+	int nr, n, total;
+
+	total = 0;
+
+	pg_duplicated = folio_test_twin(folio);
+	pg_main = folio_test_main(folio);
+
+	if (!pg_main && !pg_duplicated)
+		return;
+
+	trace_duplication_remove_mapping_hook(folio);
+
+	duplication = retrieve_struct_duplication(folio);
+	index = folio->index;
+
+	spin_lock_irqsave(&duplication->lock, flags);
+	if (folio_test_main(folio)) {
+		this_cpu_inc(duplication_stats.remove_mapping_mains);
+		duplication_for_each_twin(duplication, twin, n)
+		{
+			if (!folio_isolate_lru(twin)) {
+				// already isolated
+				folio_set_reclaim(twin);
+				continue;
+			}
+			if (!folio_ref_freeze(twin, 2)) {
+				folio_putback_lru(twin);
+				deactivate_file_folio(twin);
+				folio_set_reclaim(twin);
+				continue;
+			}
+
+			nr = folio_nr_pages(twin);
+			total += nr;
+			struct_remove_twin(twin, duplication);
+
+			// not dropping refs -> we're already done because refs are frozen to 0
+			folio_clear_active(twin);
+			folio_clear_referenced(twin);
+
+			this_cpu_inc(duplication_stats.remove_mapping_twins);
+			twin->mapping = NULL;
+
+			trace_duplication_free_duplicated(twin);
+
+			__folio_put(twin);
+		}
+		total += folio_nr_pages(folio);
+		struct_remove_main(folio, duplication);
+		trace_duplication_free_main(folio);
+	} else {
+		// update the stats on the lru
+		nr = folio_nr_pages(folio);
+		struct_remove_twin(folio, duplication);
+		total += nr;
+
+		// not dropping refs -> we're already done because refs are frozen to 0
+
+		this_cpu_inc(duplication_stats.remove_mapping_twins);
+		folio->mapping = NULL;
+
+		trace_duplication_free_duplicated(folio);
+	}
+	spin_unlock_irqrestore(&duplication->lock, flags);
+	struct_duplication_put(duplication);
+}
+
+// both folio should be locked already
+void duplication_migration_hook(struct folio *from, struct folio *to)
+{
+	struct duplication *duplication;
+	bool pg_main, pg_twin;
+
+	BUG_ON(!folio_test_locked(from));
+	BUG_ON(!folio_test_locked(to));
+
+	pg_main = folio_test_main(from);
+	pg_twin = folio_test_twin(from);
+
+	if (!pg_main && !pg_twin)
+		return;
+
+	duplication = retrieve_struct_duplication(from);
+
+	if (pg_main) {
+		BUG_ON(duplication_get_main(duplication) != from);
+		// will restore the private field of old main
+		struct_remove_main(from, duplication);
+		struct_add_main(to, duplication);
+		this_cpu_inc(duplication_stats.migrations_mains);
+	} else if (pg_twin) {
+		struct_remove_twin(from, duplication);
+		from->mapping = NULL;
+		// struct duplication is not dropping its ref on twin because it
+		// is frozen
+		folio_ref_unfreeze(from, 1);
+		// but we need to get a new ref for the new folio
+		folio_get(to);
+		struct_add_twin(to, duplication);
+		this_cpu_inc(duplication_stats.migrations_twins);
+	}
+
+	trace_duplication_migration(from, to);
+
+	struct_duplication_put(duplication);
+}
diff --git a/mm/duplication/main.c b/mm/duplication/main.c
new file mode 100644
index 000000000000..e38c81bb9867
--- /dev/null
+++ b/mm/duplication/main.c
@@ -0,0 +1,456 @@
+#include "linux/cpumask.h"
+#include "linux/kref.h"
+#include "linux/kstrtox.h"
+#include "linux/memory_hotplug.h"
+#include "linux/migrate.h"
+#include "linux/percpu-defs.h"
+#include "linux/stddef.h"
+#include "linux/topology.h"
+#include <linux/init.h>
+#include <linux/mm_types.h>
+#include <linux/page_ref.h>
+#include <linux/pagemap.h>
+#include <linux/spinlock.h>
+#include <linux/mm.h>
+#include <linux/sysfs.h>
+#include <linux/page-flags.h>
+#include <linux/printk.h>
+#include <linux/module.h>
+#include <linux/swap.h>
+#include <linux/slab.h>
+#include <linux/duplication.h>
+
+#define CREATE_TRACE_POINTS
+#include <trace/events/duplication.h>
+
+bool duplication_enable;
+bool duplication_async;
+bool duplication_switch_main_eviction;
+bool duplication_memory_pressure_mitigation;
+unsigned int duplication_threshold;
+struct kmem_cache *cache_duplication;
+
+DEFINE_PER_CPU(struct duplication_stats, duplication_stats);
+
+/******************************************************************************
+ *                          sysfs interface
+ ******************************************************************************/
+
+static ssize_t memory_pressure_mitigation_show(struct kobject *kobj, struct kobj_attribute *attr,
+			    char *buf)
+{
+	return sysfs_emit(buf, "%u\n", duplication_memory_pressure_mitigation);
+}
+
+static ssize_t memory_pressure_mitigation_store(struct kobject *kobj, struct kobj_attribute *attr,
+			     const char *buf, size_t len)
+{
+	if (tolower(*buf) == '0') {
+		duplication_memory_pressure_mitigation = false;
+	} else if (tolower(*buf) == '1') {
+		duplication_memory_pressure_mitigation = true;
+	} else
+		return -EINVAL;
+
+	return len;
+}
+static ssize_t switch_main_eviction_show(struct kobject *kobj, struct kobj_attribute *attr,
+			    char *buf)
+{
+	return sysfs_emit(buf, "%u\n", duplication_switch_main_eviction);
+}
+
+static ssize_t switch_main_eviction_store(struct kobject *kobj, struct kobj_attribute *attr,
+			     const char *buf, size_t len)
+{
+	if (tolower(*buf) == '0') {
+		duplication_switch_main_eviction = false;
+	} else if (tolower(*buf) == '1') {
+		duplication_switch_main_eviction = true;
+	} else
+		return -EINVAL;
+
+	return len;
+}
+static ssize_t enabled_show(struct kobject *kobj, struct kobj_attribute *attr,
+			    char *buf)
+{
+	return sysfs_emit(buf, "%u\n", duplication_enable);
+}
+
+static ssize_t enabled_store(struct kobject *kobj, struct kobj_attribute *attr,
+			     const char *buf, size_t len)
+{
+	if (tolower(*buf) == '0') {
+		pr_info("disabling duplication\n");
+		duplication_enable = false;
+	} else if (tolower(*buf) == '1') {
+		pr_info("enabling duplication\n");
+		duplication_enable = true;
+	} else
+		return -EINVAL;
+
+	return len;
+}
+
+static ssize_t threshold_show(struct kobject *kobj, struct kobj_attribute *attr,
+			      char *buf)
+{
+	return sysfs_emit(buf, "%u\n", duplication_threshold);
+}
+
+static ssize_t threshold_store(struct kobject *kobj,
+			       struct kobj_attribute *attr, const char *buf,
+			       size_t len)
+{
+	int ret;
+	unsigned int threshold = 0;
+	ret = kstrtouint(buf, 10, &threshold);
+	if (ret)
+		return ret;
+
+	duplication_threshold = threshold;
+	return len;
+}
+
+static ssize_t stats_store(struct kobject *kobj, struct kobj_attribute *attr,
+			   const char *buf, size_t len)
+{
+	struct duplication_stats *pcp_d_stats;
+	int i;
+
+	for_each_possible_cpu(i) {
+		pcp_d_stats = &per_cpu(duplication_stats, i);
+		pcp_d_stats->truncates = 0;
+		pcp_d_stats->migrations_mains = 0;
+		pcp_d_stats->migrations_twins = 0;
+		pcp_d_stats->remove_mapping_mains = 0;
+		pcp_d_stats->remove_mapping_twins = 0;
+		pcp_d_stats->switch_main = 0;
+		pcp_d_stats->switch_main_write = 0;
+		pcp_d_stats->local_read = 0;
+		pcp_d_stats->local_write = 0;
+		pcp_d_stats->distant_read = 0;
+		pcp_d_stats->distant_write = 0;
+	}
+
+	return len;
+}
+
+static ssize_t stats_show(struct kobject *kobj, struct kobj_attribute *attr,
+			  char *buf)
+{
+	struct duplication_stats d_stats = {0};
+	struct duplication_stats *pcp_d_stats;
+	int i;
+
+	for_each_possible_cpu(i) {
+		pcp_d_stats = &per_cpu(duplication_stats, i);
+		d_stats.nr_struct_duplication +=
+			pcp_d_stats->nr_struct_duplication;
+		d_stats.mains += pcp_d_stats->mains;
+		d_stats.twins += pcp_d_stats->twins;
+		d_stats.truncates += pcp_d_stats->truncates;
+		d_stats.migrations_mains += pcp_d_stats->migrations_mains;
+		d_stats.migrations_twins += pcp_d_stats->migrations_twins;
+		d_stats.remove_mapping_mains +=
+			pcp_d_stats->remove_mapping_mains;
+		d_stats.remove_mapping_twins +=
+			pcp_d_stats->remove_mapping_twins;
+		d_stats.switch_main += pcp_d_stats->switch_main;
+		d_stats.switch_main_write += pcp_d_stats->switch_main_write;
+		d_stats.local_read += pcp_d_stats->local_read;
+		d_stats.local_write += pcp_d_stats->local_write;
+		d_stats.distant_read += pcp_d_stats->distant_read;
+		d_stats.distant_write += pcp_d_stats->distant_write;
+	}
+
+	return sysfs_emit(
+		buf,
+		"%lu struct duplication\n%lu mains\n%lu twins\ntruncates %lu\nmigrations main %lu\nmigrations twin %lu\nswitch main %lu\nswitch main write %lu\nremove mapping main %lu\nremove mapping twin %lu\nlocal read %lu\nlocal write %lu\ndistant read %lu\ndistant write %lu\n",
+		d_stats.nr_struct_duplication, d_stats.mains, d_stats.twins,
+		d_stats.truncates,
+		d_stats.migrations_mains - d_stats.switch_main -
+			d_stats.switch_main_write,
+		d_stats.migrations_twins, d_stats.switch_main,
+		d_stats.switch_main_write, d_stats.remove_mapping_mains,
+		d_stats.remove_mapping_twins, d_stats.local_read,
+		d_stats.local_write, d_stats.distant_read,
+		d_stats.distant_write);
+}
+
+static ssize_t dump_show(struct kobject *kobj, struct kobj_attribute *attr,
+			 char *buf)
+{
+	struct folio *folio;
+	unsigned long pfn = 0;
+	while (pfn < 131072000) {
+		folio = (struct folio *)pfn_to_online_page(pfn);
+		if (folio) {
+			if (folio_test_main(folio) || folio_test_twin(folio) ||
+			    folio_test_oldmain(folio) ||
+			    folio_test_oldtwin(folio))
+				quick_print_folio(folio, true, "found folio: ");
+
+			pfn += folio_nr_pages(folio);
+		} else {
+			pfn++;
+		}
+	}
+	return sysfs_emit(buf, "look at dmesg");
+}
+
+static struct kobj_attribute duplication_switch_main_attr = __ATTR_RW(switch_main_eviction);
+static struct kobj_attribute duplication_memory_pressure_mitigation_attr = __ATTR_RW(memory_pressure_mitigation);
+static struct kobj_attribute duplication_enabled_attr = __ATTR_RW(enabled);
+static struct kobj_attribute duplication_stats_attr = __ATTR_RW(stats);
+static struct kobj_attribute duplication_dump_attr = __ATTR_RO(dump);
+static struct kobj_attribute duplication_threshold_attr = __ATTR_RW(threshold);
+
+static struct attribute *duplication_attrs[] = {
+	&duplication_enabled_attr.attr,
+	&duplication_stats_attr.attr,
+	&duplication_dump_attr.attr,
+	&duplication_threshold_attr.attr,
+	&duplication_switch_main_attr.attr,
+	&duplication_memory_pressure_mitigation_attr.attr,
+	NULL
+};
+
+static const struct attribute_group duplication_attr_group = {
+	.name = "duplication",
+	.attrs = duplication_attrs,
+};
+
+static int __init init_duplication(void)
+{
+	int i;
+	struct duplication_stats *pcp_stats;
+	duplication_enable = false;
+	duplication_threshold = 10;
+	duplication_async = false;
+	duplication_switch_main_eviction = true;
+	duplication_memory_pressure_mitigation = true;
+	duplication_debugfs();
+	cache_duplication = kmem_cache_create("cache_duplication", (sizeof(struct duplication) + nr_online_nodes * sizeof(struct folio *)), NULL, 0);
+	if (!cache_duplication) {
+		pr_err("ERROR creating the slab cache!!!!!!!!!!!!!!!!");
+		BUG();
+	}
+
+	for_each_online_cpu(i) {
+		pcp_stats = per_cpu_ptr(&duplication_stats, i);
+		pcp_stats->nr_struct_duplication = 0;
+		pcp_stats->mains = 0;
+		pcp_stats->twins = 0;
+	}
+
+	if (sysfs_create_group(mm_kobj, &duplication_attr_group))
+		pr_err("lru_gen: failed to create sysfs group\n");
+
+	return 0;
+};
+late_initcall(init_duplication);
+
+struct folio *duplication_folio_for_writing(struct folio *folio)
+{
+	struct duplication *duplication;
+	struct folio *f = NULL;
+	unsigned long flags;
+	int f_nid, this_nid, nid;
+
+	if (!duplication_enable || !folio_test_main(folio))
+		goto out;
+
+	f_nid = folio_nid(folio);
+	this_nid = numa_node_id();
+
+	duplication = retrieve_struct_duplication(folio);
+
+	trace_duplication_write_folio(folio);
+
+	spin_lock_irqsave(&duplication->lock, flags);
+
+	if (this_nid == f_nid) {
+		duplication_reset_counters(duplication);
+	} else {
+		duplication_increment_counter(this_nid, duplication);
+		if (duplication_get_counter(this_nid, duplication) >=
+		    duplication_threshold) {
+			if (folio_test_writeback(folio) ||
+			    !folio_trylock(folio))
+				goto invalidate;
+
+			if (duplication_try_switch_main(folio, this_nid)) {
+				folio_get(duplication_get_main(duplication));
+				folio_unlock(folio);
+				folio_put(folio);
+				folio = duplication_get_main(duplication);
+				duplication_reset_counters(duplication);
+			} else
+				folio_unlock(folio);
+		}
+	}
+
+invalidate:
+
+	/* invalidate all duplicates */
+	duplication_for_each_twin(duplication, f, nid)
+	{
+#ifdef DEBUG
+		if (!folio_test_twin(f)) {
+			quick_print_struct_duplication(
+				duplication, true,
+				"duplicate is not duplicated");
+			BUG();
+		}
+#endif
+		trace_duplication_invalidate_duplicated(f);
+		// duplicate_unset_uptodate(f, duplication);
+		folio_clear_uptodate(f);
+	}
+
+	spin_unlock_irqrestore(&duplication->lock, flags);
+	struct_duplication_put(duplication);
+
+out:
+	if (folio_nid(folio) == numa_node_id())
+		this_cpu_inc(duplication_stats.local_write);
+	else
+		this_cpu_inc(duplication_stats.distant_write);
+
+	return folio;
+}
+
+static struct duplication *alloc_struct_duplication(void)
+{
+	struct duplication *duplication;
+
+	duplication = kmem_cache_alloc(cache_duplication, (GFP_KERNEL & ~__GFP_RECLAIM) | __GFP_NOWARN);
+	if (!duplication)
+		goto out;
+
+	duplication->main = -1;
+	kref_init(&duplication->ref);
+	spin_lock_init(&duplication->lock);
+
+out:
+	return duplication;
+}
+
+static bool is_under_memory_pressure(int nid) {
+	struct zone *zone;
+	struct pglist_data *pgdat;
+
+	if (!duplication_memory_pressure_mitigation)
+		return false;
+
+	pgdat = NODE_DATA(nid);
+	zone = &pgdat->node_zones[ZONE_NORMAL];
+
+	return zone_page_state(zone, NR_FREE_PAGES) <= high_wmark_pages(zone);
+}
+
+struct folio *duplication_find_or_duplicate(struct folio *folio,
+					    int current_nid)
+{
+	struct folio *twin = NULL;
+	struct duplication *duplication;
+	bool local;
+	unsigned long flags;
+
+	local = folio_nid(folio) == current_nid;
+
+	if (local) {
+		trace_duplication_local_node(folio);
+		goto out;
+	}
+	/* folio mapped means that the folio is inside of a process' page table */
+	if (folio_test_anon(folio) || folio_test_swapcache(folio) ||
+	    folio_mapcount(folio) > 0 || folio_test_reclaim(folio) ||
+	    !duplication_enable) {
+		trace_duplication_cannot_duplicate(folio);
+		goto out;
+	}
+
+	if (!folio_trylock(folio))
+		// folio is currently in use
+		goto out;
+
+	if (!folio_test_main(folio) && !folio_test_twin(folio)) {
+		if (is_under_memory_pressure(current_nid))
+			goto unlock_folio;
+
+		duplication = alloc_struct_duplication();
+		if (!duplication) {
+			trace_duplication_failed_allocation_struct_duplication(
+				folio);
+			// pr_err("failed to allocate struct duplication\n");
+			goto unlock_folio;
+		}
+		this_cpu_inc(duplication_stats.nr_struct_duplication);
+#ifdef DUPLICATION_DEBUG
+		BUG_ON(folio_test_twin(folio));
+#endif
+		struct_add_main(folio, duplication);
+	} else {
+		duplication = retrieve_struct_duplication(folio);
+	}
+
+	spin_lock_irqsave(&duplication->lock, flags);
+	twin = find_twin(duplication, current_nid);
+	if (twin) {
+		if (!folio_try_get(twin)) {
+			twin = NULL;
+			goto unlock_struct;
+		}
+		if (folio_nr_pages(twin) != folio_nr_pages(folio)) {
+			pr_err("inconsistent folio nr pages %ld %ld",
+			       folio_nr_pages(folio), folio_nr_pages(twin));
+			quick_print_folio(folio, true, "the main");
+			quick_print_folio(twin, true, "the twin");
+			folio_put(twin);
+			twin = NULL;
+			goto unlock_struct;
+		}
+
+		trace_duplication_duplicate_found(folio, twin);
+
+		if (!folio_test_uptodate(twin)) {
+			trace_duplication_duplicate_out_of_date(twin);
+			if (!duplication_set_twin_uptodate(twin, duplication)) {
+				folio_put(twin);
+				twin = NULL;
+				goto unlock_struct;
+			}
+		}
+
+		// don't drop the ref here, it will be drop later by the read
+	} else if (!is_under_memory_pressure(current_nid)) {
+		twin = folio_try_duplicate(folio, current_nid, duplication);
+	}
+
+unlock_struct:
+	spin_unlock_irqrestore(&duplication->lock, flags);
+	struct_duplication_put(duplication);
+unlock_folio:
+	folio_unlock(folio);
+	if (twin) {
+		// we switch the returned folio
+		// put current folio because we don't need it anymore
+		folio_put(folio);
+		this_cpu_inc(duplication_stats.local_read);
+		return twin;
+	}
+out:
+	if (local)
+		this_cpu_inc(duplication_stats.local_read);
+	else
+		this_cpu_inc(duplication_stats.distant_read);
+
+	return folio;
+}
+
+MODULE_LICENSE("GPL");
+MODULE_AUTHOR("Jérôme Coquisart, <coquisart@os.rwth-aachen.de>");
+MODULE_DESCRIPTION("linux page cache duplication");
diff --git a/mm/filemap.c b/mm/filemap.c
index 05adf0392625..3b6a7a6e48e1 100644
--- a/mm/filemap.c
+++ b/mm/filemap.c
@@ -10,7 +10,6 @@
  * most "normal" filesystems (but you don't /have/ to use this:
  * the NFS filesystem used to do this differently, for example)
  */
-#include <linux/export.h>
 #include <linux/compiler.h>
 #include <linux/dax.h>
 #include <linux/fs.h>
@@ -49,6 +48,7 @@
 #include <linux/sched/mm.h>
 #include <asm/pgalloc.h>
 #include <asm/tlbflush.h>
+#include <linux/duplication.h>
 #include "internal.h"
 
 #define CREATE_TRACE_POINTS
@@ -232,6 +232,7 @@ void __filemap_remove_folio(struct folio *folio, void *shadow)
 	filemap_unaccount_folio(mapping, folio);
 	page_cache_delete(mapping, folio, shadow);
 }
+EXPORT_SYMBOL_GPL(__filemap_remove_folio);
 
 void filemap_free_folio(struct address_space *mapping, struct folio *folio)
 {
@@ -956,6 +957,7 @@ noinline int __filemap_add_folio(struct address_space *mapping,
 	folio_put_refs(folio, nr);
 	return xas_error(&xas);
 }
+EXPORT_SYMBOL_GPL(__filemap_add_folio);
 ALLOW_ERROR_INJECTION(__filemap_add_folio, ERRNO);
 
 int filemap_add_folio(struct address_space *mapping, struct folio *folio,
@@ -1890,6 +1892,11 @@ struct folio *__filemap_get_folio(struct address_space *mapping, pgoff_t index,
 	if (!folio)
 		goto no_page;
 
+	if (fgp_flags & FGP_WRITE)
+		folio = duplication_folio_for_writing(folio);
+	else if (fgp_flags & FGP_ACCESSED)
+		folio = duplication_find_or_duplicate(folio, numa_node_id());
+
 	if (fgp_flags & FGP_LOCK) {
 		if (fgp_flags & FGP_NOWAIT) {
 			if (!folio_trylock(folio)) {
@@ -2323,6 +2330,8 @@ static void filemap_get_read_batch(struct address_space *mapping,
 {
 	XA_STATE(xas, &mapping->i_pages, index);
 	struct folio *folio;
+	int nid = numa_node_id();
+	int i;
 
 	rcu_read_lock();
 	for (folio = xas_load(&xas); folio; folio = xas_next(&xas)) {
@@ -2345,6 +2354,7 @@ static void filemap_get_read_batch(struct address_space *mapping,
 		if (folio_test_readahead(folio))
 			break;
 		xas_advance(&xas, folio_next_index(folio) - 1);
+
 		continue;
 put_folio:
 		folio_put(folio);
@@ -2352,6 +2362,11 @@ static void filemap_get_read_batch(struct address_space *mapping,
 		xas_reset(&xas);
 	}
 	rcu_read_unlock();
+	for (i = 0; i < folio_batch_count(fbatch); i++) {
+		// replace the recently added folio to the fbatch
+		folio = fbatch->folios[i];
+		fbatch->folios[i] = duplication_find_or_duplicate(folio, nid);
+	}
 }
 
 static int filemap_read_folio(struct file *file, filler_t filler,
@@ -2573,6 +2588,7 @@ static int filemap_get_pages(struct kiocb *iocb, size_t count,
 					  need_uptodate);
 		if (err)
 			goto err;
+
 	}
 
 	trace_mm_filemap_get_pages(mapping, index, last_index - 1);
diff --git a/mm/internal.h b/mm/internal.h
index 398633d6b6c9..9121185b3bd5 100644
--- a/mm/internal.h
+++ b/mm/internal.h
@@ -455,8 +455,8 @@ static inline bool folio_needs_release(struct folio *folio)
 {
 	struct address_space *mapping = folio_mapping(folio);
 
-	return folio_has_private(folio) ||
-		(mapping && mapping_release_always(mapping));
+	return !folio_test_twin(folio) && (folio_has_private(folio) ||
+		(mapping && mapping_release_always(mapping)));
 }
 
 extern unsigned long highest_memmap_pfn;
diff --git a/mm/khugepaged.c b/mm/khugepaged.c
index b538c3d48386..fe77adb5eb2b 100644
--- a/mm/khugepaged.c
+++ b/mm/khugepaged.c
@@ -1,4 +1,5 @@
 // SPDX-License-Identifier: GPL-2.0
+#include "linux/duplication.h"
 #define pr_fmt(fmt) KBUILD_MODNAME ": " fmt
 
 #include <linux/mm.h>
@@ -2183,6 +2184,11 @@ static int collapse_file(struct mm_struct *mm, unsigned long addr,
 	 */
 	list_for_each_entry_safe(folio, tmp, &pagelist, lru) {
 		list_del(&folio->lru);
+		if (folio_test_main(folio)) {
+			quick_print_folio(folio, true, "collapsed");
+			pr_err("%s", file->f_path.dentry->d_name.name);
+		}
+		duplication_eviction(folio, false);
 		folio->mapping = NULL;
 		folio_clear_active(folio);
 		folio_clear_unevictable(folio);
diff --git a/mm/memcontrol-v1.c b/mm/memcontrol-v1.c
index f8744f5630bb..be78a0d1bf92 100644
--- a/mm/memcontrol-v1.c
+++ b/mm/memcontrol-v1.c
@@ -2716,6 +2716,7 @@ static int memcg_numa_stat_show(struct seq_file *m, void *v)
 
 static const unsigned int memcg1_stats[] = {
 	NR_FILE_PAGES,
+	NR_FILE_TWINS,
 	NR_ANON_MAPPED,
 #ifdef CONFIG_TRANSPARENT_HUGEPAGE
 	NR_ANON_THPS,
diff --git a/mm/memcontrol.c b/mm/memcontrol.c
index ae1d184d035a..4e23236f4e8a 100644
--- a/mm/memcontrol.c
+++ b/mm/memcontrol.c
@@ -292,6 +292,7 @@ static const unsigned int memcg_node_stat_items[] = {
 	NR_ANON_MAPPED,
 	NR_FILE_MAPPED,
 	NR_FILE_PAGES,
+	NR_FILE_TWINS,
 	NR_FILE_DIRTY,
 	NR_WRITEBACK,
 	NR_SHMEM,
@@ -1325,6 +1326,7 @@ struct memory_stat {
 static const struct memory_stat memory_stats[] = {
 	{ "anon",			NR_ANON_MAPPED			},
 	{ "file",			NR_FILE_PAGES			},
+	{ "twins",			NR_FILE_TWINS			},
 	{ "kernel",			MEMCG_KMEM			},
 	{ "kernel_stack",		NR_KERNEL_STACK_KB		},
 	{ "pagetables",			NR_PAGETABLE			},
@@ -4494,6 +4496,7 @@ int __mem_cgroup_charge(struct folio *folio, struct mm_struct *mm, gfp_t gfp)
 
 	return ret;
 }
+EXPORT_SYMBOL_GPL(__mem_cgroup_charge);
 
 /**
  * mem_cgroup_hugetlb_try_charge - try to charge the memcg for a hugetlb folio
diff --git a/mm/memory.c b/mm/memory.c
index 525f96ad65b8..cb3a6dc49a5f 100644
--- a/mm/memory.c
+++ b/mm/memory.c
@@ -40,6 +40,9 @@
  * Aug/Sep 2004 Changed to four level page tables (Andi Kleen)
  */
 
+#include "linux/duplication.h"
+#include "linux/mm_types.h"
+#include "linux/page-flags.h"
 #include <linux/kernel_stat.h>
 #include <linux/mm.h>
 #include <linux/mm_inline.h>
@@ -4320,7 +4323,7 @@ vm_fault_t do_swap_page(struct vm_fault *vmf)
 				/* To provide entry to swap_read_folio() */
 				folio->swap = entry;
 				swap_read_folio(folio, NULL);
-				folio->private = NULL;
+				folio_change_private(folio, NULL);
 			}
 		} else {
 			folio = swapin_readahead(entry, GFP_HIGHUSER_MOVABLE,
diff --git a/mm/migrate.c b/mm/migrate.c
index dfa24e41e8f9..5a84f0040071 100644
--- a/mm/migrate.c
+++ b/mm/migrate.c
@@ -13,6 +13,12 @@
  * Christoph Lameter
  */
 
+#include "linux/mm.h"
+#include "linux/mm_types.h"
+#include "linux/page-flags.h"
+#include "linux/printk.h"
+#include "linux/spinlock.h"
+#include "linux/stddef.h"
 #include <linux/migrate.h>
 #include <linux/export.h>
 #include <linux/swap.h>
@@ -44,6 +50,8 @@
 #include <linux/sched/sysctl.h>
 #include <linux/memory-tiers.h>
 #include <linux/pagewalk.h>
+#include <linux/duplication.h>
+#include <trace/events/duplication.h>
 
 #include <asm/tlbflush.h>
 
@@ -457,6 +465,10 @@ static int folio_expected_refs(struct address_space *mapping,
 		struct folio *folio)
 {
 	int refs = 1;
+
+	if (folio_test_twin(folio))
+		return refs + 1;
+
 	if (!mapping)
 		return refs;
 
@@ -502,6 +514,15 @@ static int __folio_migrate_mapping(struct address_space *mapping,
 		if (folio_test_swapbacked(folio))
 			__folio_set_swapbacked(newfolio);
 
+		/*
+		 * Will be unfrozen from migrate_flags
+		 * by the duplication migration hook
+		 */
+		if (folio_test_twin(folio)) {
+			if (!folio_ref_freeze(folio, expected_count))
+				return -EAGAIN;
+		}
+
 		return MIGRATEPAGE_SUCCESS;
 	}
 
@@ -530,7 +551,7 @@ static int __folio_migrate_mapping(struct address_space *mapping,
 		__folio_set_swapbacked(newfolio);
 		if (folio_test_swapcache(folio)) {
 			folio_set_swapcache(newfolio);
-			newfolio->private = folio_get_private(folio);
+			folio_change_private(newfolio, folio_get_private(folio));
 		}
 		entries = nr;
 	} else {
@@ -677,6 +698,9 @@ void folio_migrate_flags(struct folio *newfolio, struct folio *folio)
 		folio_set_workingset(newfolio);
 	if (folio_test_checked(folio))
 		folio_set_checked(newfolio);
+
+	duplication_migration_hook(folio, newfolio);
+
 	/*
 	 * PG_anon_exclusive (-> PG_mappedtodisk) is always migrated via
 	 * migration entries. We can still have PG_anon_exclusive set on an
@@ -726,7 +750,7 @@ void folio_migrate_flags(struct folio *newfolio, struct folio *folio)
 
 	/* page->private contains hugetlb specific flags */
 	if (!folio_test_hugetlb(folio))
-		folio->private = NULL;
+		folio_change_private(folio, NULL);
 
 	/*
 	 * If any waiters have accumulated on the new page then
@@ -761,17 +785,24 @@ static int __migrate_folio(struct address_space *mapping, struct folio *dst,
 	int rc, expected_count = folio_expected_refs(mapping, src);
 
 	/* Check whether src does not have extra refs before we do more work */
-	if (folio_ref_count(src) != expected_count)
+	if (folio_ref_count(src) != expected_count) 
 		return -EAGAIN;
 
-	rc = folio_mc_copy(dst, src);
-	if (unlikely(rc))
-		return rc;
+	if (!folio_test_main(src) || !folio_test_uptodate(dst)) {
+		rc = folio_mc_copy(dst, src);
+		if (unlikely(rc)) 
+			return rc;
+	}
 
-	rc = __folio_migrate_mapping(mapping, dst, src, expected_count);
-	if (rc != MIGRATEPAGE_SUCCESS)
+	if (folio_test_twin(src)) 
+		rc = __folio_migrate_mapping(NULL, dst, src, expected_count);
+	else 
+		rc = __folio_migrate_mapping(mapping, dst, src, expected_count);
+
+	if (rc != MIGRATEPAGE_SUCCESS) 
 		return rc;
 
+
 	if (src_private)
 		folio_attach_private(dst, folio_detach_private(src));
 
@@ -1110,6 +1141,94 @@ static int move_to_new_folio(struct folio *dst, struct folio *src,
 	return rc;
 }
 
+bool duplication_try_switch_main(struct folio *folio, int nid)
+{
+	struct duplication *duplication = NULL;
+	unsigned long flags;
+	int n;
+	int ret = -1;
+	bool r = false;
+	bool copy = true;
+	struct address_space *mapping = folio_mapping(folio);
+	struct folio *twin;
+
+	duplication = retrieve_struct_duplication(folio);
+
+	if (nid == -1)
+		spin_lock_irqsave(&duplication->lock, flags);
+	duplication_for_each_twin(duplication, twin, n) {
+		if (nid != -1 && n != nid) {
+			twin = NULL;
+			continue;
+		}
+
+		if (!folio_test_lru(twin))  {
+			// already isolated
+			twin = NULL;
+			continue;
+		}
+		if (!folio_trylock(twin)) {
+			twin = NULL;
+			continue;
+		}
+		if (!folio_ref_freeze(twin, 1)) {
+			folio_unlock(twin);
+			twin = NULL;
+			continue;
+		}
+
+		// call that before remove twin because it's going to remove
+		// this flag
+		if (folio_test_uptodate(twin)) 
+			copy = false;
+		struct_remove_twin(twin, duplication);
+		break;
+	}
+
+	if (!twin) {
+		goto unlock_struct;
+	}
+
+	if (folio_nr_pages(twin) != folio_nr_pages(folio)) {
+		pr_err("%ld %ld", folio_nr_pages(folio), folio_nr_pages(twin));
+		quick_print_folio(folio, true, "the main");
+		quick_print_folio(twin, true, "the twin");
+		BUG();
+	}
+	if (copy)
+		folio_clear_uptodate(twin);
+
+	ret = move_to_new_folio(twin, folio, MIGRATE_SYNC);
+	if (ret != MIGRATEPAGE_SUCCESS) {
+		r = false;
+		twin->mapping = NULL;
+		folio_unlock(twin);
+		__folio_put(twin);
+		goto unlock_struct;
+	}
+	r = true;
+
+	if (nid != -1) {
+		folio_get(folio);
+		folio->mapping = mapping;
+		mem_cgroup_charge(folio, NULL, mapping_gfp_constraint(mapping, GFP_KERNEL));
+		struct_add_twin(folio, duplication);
+		this_cpu_inc(duplication_stats.switch_main_write);
+	} else {
+		folio_unlock(folio);
+		this_cpu_inc(duplication_stats.switch_main);
+	}
+
+	folio_unlock(twin);
+	trace_duplication_switch_main(folio, twin);
+
+unlock_struct:
+	if (nid == -1)
+		spin_unlock_irqrestore(&duplication->lock, flags);
+	struct_duplication_put(duplication);
+	return r;
+}
+
 /*
  * To record some information during migration, we use unused private
  * field of struct folio of the newly allocated destination folio.
@@ -1132,11 +1251,11 @@ static void __migrate_folio_extract(struct folio *dst,
 				   int *old_page_state,
 				   struct anon_vma **anon_vmap)
 {
-	unsigned long private = (unsigned long)dst->private;
+	unsigned long private = (unsigned long)folio_get_private(dst);
 
 	*anon_vmap = (struct anon_vma *)(private & ~PAGE_OLD_STATES);
 	*old_page_state = private & PAGE_OLD_STATES;
-	dst->private = NULL;
+	folio_change_private(dst, NULL);
 }
 
 /* Restore the source folio to the original state upon failure */
@@ -1359,12 +1478,27 @@ static int migrate_folio_move(free_folio_t put_new_folio, unsigned long private,
 	struct anon_vma *anon_vma = NULL;
 	bool is_lru = !__folio_test_movable(src);
 	struct list_head *prev;
+	struct duplication *duplication;
+	bool struct_duplication = false;
+	unsigned long flags;
 
 	__migrate_folio_extract(dst, &old_page_state, &anon_vma);
 	prev = dst->lru.prev;
 	list_del(&dst->lru);
 
+	if (folio_test_main(src) || folio_test_twin(src)) {
+		struct_duplication = true;
+		duplication = retrieve_struct_duplication(src);
+		spin_lock_irqsave(&duplication->lock, flags);
+	}
+
 	rc = move_to_new_folio(dst, src, mode);
+
+	if (struct_duplication) {
+		spin_unlock_irqrestore(&duplication->lock, flags);
+		struct_duplication_put(duplication);
+	}
+
 	if (rc)
 		goto out;
 
@@ -1563,6 +1697,11 @@ static inline int try_split_folio(struct folio *folio, struct list_head *split_f
 {
 	int rc;
 
+	// do not split main/twin folios
+	if (folio_test_twin(folio) || folio_test_main(folio)) {
+		return -1;
+	}
+
 	if (mode == MIGRATE_ASYNC) {
 		if (!folio_trylock(folio))
 			return -EAGAIN;
diff --git a/mm/page_alloc.c b/mm/page_alloc.c
index e0a77fe1b630..cf55cfd407fd 100644
--- a/mm/page_alloc.c
+++ b/mm/page_alloc.c
@@ -15,6 +15,7 @@
  *          (lots of bits borrowed from Ingo Molnar & Andrew Morton)
  */
 
+#include "linux/mm_types.h"
 #include <linux/stddef.h>
 #include <linux/mm.h>
 #include <linux/highmem.h>
@@ -55,6 +56,7 @@
 #include <linux/delayacct.h>
 #include <linux/cacheinfo.h>
 #include <linux/pgalloc_tag.h>
+#include <linux/duplication.h>
 #include <asm/div64.h>
 #include "internal.h"
 #include "shuffle.h"
@@ -495,6 +497,8 @@ static void bad_page(struct page *page, const char *reason)
 
 	pr_alert("BUG: Bad page state in process %s  pfn:%05lx\n",
 		current->comm, page_to_pfn(page));
+	quick_print_folio(page_folio(page), true, "");
+	pr_alert("Reason: %s\n", reason);
 	dump_page(page, reason);
 
 	print_modules();
@@ -892,10 +896,56 @@ static const char *page_bad_reason(struct page *page, unsigned long flags)
 	if (unlikely(page_ref_count(page) != 0))
 		bad_reason = "nonzero _refcount";
 	if (unlikely(page->flags & flags)) {
-		if (flags == PAGE_FLAGS_CHECK_AT_PREP)
-			bad_reason = "PAGE_FLAGS_CHECK_AT_PREP flag(s) set";
-		else
-			bad_reason = "PAGE_FLAGS_CHECK_AT_FREE flag(s) set";
+		if (flags == PAGE_FLAGS_CHECK_AT_PREP){
+			unsigned long flags = page->flags;
+			if (flags & (1UL << PG_lru))
+				bad_reason = "PAGE_FLAGS_CHECK_AT_PREP flag(s) LRU";
+			if (flags & (1UL << PG_locked))
+				bad_reason = "PAGE_FLAGS_CHECK_AT_PREP flag(s) locked";
+			if (flags & (1UL << PG_private))
+				bad_reason = "PAGE_FLAGS_CHECK_AT_PREP flag(s) private";
+			if (flags & (1UL << PG_private_2))
+				bad_reason = "PAGE_FLAGS_CHECK_AT_PREP flag(s) private2";
+			if (flags & (1UL << PG_writeback))
+				bad_reason = "PAGE_FLAGS_CHECK_AT_PREP flag(s) writeback";
+			if (flags & (1UL << PG_reserved))
+				bad_reason = "PAGE_FLAGS_CHECK_AT_PREP flag(s) reserved";
+			if (flags & (1UL << PG_active))
+				bad_reason = "PAGE_FLAGS_CHECK_AT_PREP flag(s) active";
+			if (flags & (1UL << PG_unevictable))
+				bad_reason = "PAGE_FLAGS_CHECK_AT_PREP flag(s) unevictable";
+			if (flags & (__PG_MLOCKED))
+				bad_reason = "PAGE_FLAGS_CHECK_AT_PREP flag(s) mlocked";
+			if (flags & (LRU_GEN_MASK))
+				bad_reason = "PAGE_FLAGS_CHECK_AT_PREP flag(s) lrugen";
+			else
+				bad_reason = "PAGE_FLAGS_CHECK_AT_PREP flag(s) set";
+		}
+		else if (page->flags & PAGE_FLAGS_CHECK_AT_FREE) {
+			unsigned long flags = page->flags;
+			if (flags & (1UL << PG_lru))
+				bad_reason = "PAGE_FLAGS_CHECK_AT_FREE flag(s) LRU";
+			if (flags & (1UL << PG_locked))
+				bad_reason = "PAGE_FLAGS_CHECK_AT_FREE flag(s) locked";
+			if (flags & (1UL << PG_private))
+				bad_reason = "PAGE_FLAGS_CHECK_AT_FREE flag(s) private";
+			if (flags & (1UL << PG_private_2))
+				bad_reason = "PAGE_FLAGS_CHECK_AT_FREE flag(s) private2";
+			if (flags & (1UL << PG_writeback))
+				bad_reason = "PAGE_FLAGS_CHECK_AT_FREE flag(s) writeback";
+			if (flags & (1UL << PG_reserved))
+				bad_reason = "PAGE_FLAGS_CHECK_AT_FREE flag(s) reserved";
+			if (flags & (1UL << PG_active))
+				bad_reason = "PAGE_FLAGS_CHECK_AT_FREE flag(s) active";
+			if (flags & (1UL << PG_unevictable))
+				bad_reason = "PAGE_FLAGS_CHECK_AT_FREE flag(s) unevictable";
+			if (flags & (__PG_MLOCKED))
+				bad_reason = "PAGE_FLAGS_CHECK_AT_FREE flag(s) mlocked";
+			if (flags & (LRU_GEN_MASK))
+				bad_reason = "PAGE_FLAGS_CHECK_AT_FREE flag(s) lrugen";
+		} else {
+				bad_reason = "there is a problem with the flags lol";
+		}
 	}
 #ifdef CONFIG_MEMCG
 	if (unlikely(page->memcg_data))
@@ -2714,7 +2764,7 @@ void free_unref_folios(struct folio_batch *folios)
 				      pfn, order, FPI_NONE);
 			continue;
 		}
-		folio->private = (void *)(unsigned long)order;
+		folio_change_private(folio, (void *)(unsigned long)order);
 		if (j != i)
 			folios->folios[j] = folio;
 		j++;
@@ -2725,10 +2775,10 @@ void free_unref_folios(struct folio_batch *folios)
 		struct folio *folio = folios->folios[i];
 		struct zone *zone = folio_zone(folio);
 		unsigned long pfn = folio_pfn(folio);
-		unsigned int order = (unsigned long)folio->private;
+		unsigned int order = (unsigned long)folio_get_private(folio);
 		int migratetype;
 
-		folio->private = NULL;
+		folio_change_private(folio, NULL);
 		migratetype = get_pfnblock_migratetype(&folio->page, pfn);
 
 		/* Different zone requires a different pcp lock */
diff --git a/mm/page_io.c b/mm/page_io.c
index 01749b99fb54..f02217ec756d 100644
--- a/mm/page_io.c
+++ b/mm/page_io.c
@@ -25,6 +25,7 @@
 #include <linux/sched/task.h>
 #include <linux/delayacct.h>
 #include <linux/zswap.h>
+#include <linux/duplication.h>
 #include "swap.h"
 
 static void __end_swap_bio_write(struct bio *bio)
diff --git a/mm/readahead.c b/mm/readahead.c
index bf79275060f3..dc749b4fc90e 100644
--- a/mm/readahead.c
+++ b/mm/readahead.c
@@ -113,6 +113,7 @@
  * ->read_folio() which may be less efficient.
  */
 
+#include "linux/mmdebug.h"
 #include <linux/blkdev.h>
 #include <linux/kernel.h>
 #include <linux/dax.h>
diff --git a/mm/swap.c b/mm/swap.c
index 59f30a981c6f..20a6058a2bdd 100644
--- a/mm/swap.c
+++ b/mm/swap.c
@@ -14,6 +14,10 @@
  * Buffermem limits added 12.3.98, Rik van Riel.
  */
 
+#include "linux/duplication.h"
+#include "linux/page-flags.h"
+#include "linux/printk.h"
+#include "linux/stddef.h"
 #include <linux/mm.h>
 #include <linux/sched.h>
 #include <linux/kernel_stat.h>
@@ -96,6 +100,22 @@ static void page_cache_release(struct folio *folio)
 
 void __folio_put(struct folio *folio)
 {
+	if (folio_test_main(folio)) {
+		quick_print_folio(folio, true, "folio put main");
+		struct duplication *duplication = retrieve_struct_duplication(folio);
+		if (!duplication) 
+			pr_err("no struct duplication");
+
+		quick_print_struct_duplication(duplication, false, "folio put");
+	}
+	if (folio_test_twin(folio)) {
+		quick_print_folio(folio, true, "folio put twin");
+		struct duplication *duplication = retrieve_struct_duplication(folio);
+		if (!duplication) 
+			pr_err("no struct duplication");
+
+		quick_print_struct_duplication(duplication, false, "folio put");
+	}
 	if (unlikely(folio_is_zone_device(folio))) {
 		free_zone_device_folio(folio);
 		return;
@@ -339,6 +359,8 @@ static void lru_activate(struct lruvec *lruvec, struct folio *folio)
 	lruvec_add_folio(lruvec, folio);
 	trace_mm_lru_activate(folio);
 
+	WARN_ONCE(nr_pages <= 0, "wtffff");
+
 	__count_vm_events(PGACTIVATE, nr_pages);
 	__count_memcg_events(lruvec_memcg(lruvec), PGACTIVATE, nr_pages);
 }
diff --git a/mm/truncate.c b/mm/truncate.c
index 0668cd340a46..3144409029f5 100644
--- a/mm/truncate.c
+++ b/mm/truncate.c
@@ -8,6 +8,9 @@
  *		Initial version.
  */
 
+#include "linux/page-flags.h"
+#include "linux/printk.h"
+#include "linux/stddef.h"
 #include <linux/kernel.h>
 #include <linux/backing-dev.h>
 #include <linux/dax.h>
@@ -21,6 +24,7 @@
 #include <linux/task_io_accounting_ops.h>
 #include <linux/shmem_fs.h>
 #include <linux/rmap.h>
+#include <linux/duplication.h>
 #include "internal.h"
 
 /*
@@ -338,11 +342,15 @@ void truncate_inode_pages_range(struct address_space *mapping,
 	while (index < end && find_lock_entries(mapping, &index, end - 1,
 			&fbatch, indices)) {
 		truncate_folio_batch_exceptionals(mapping, &fbatch, indices);
-		for (i = 0; i < folio_batch_count(&fbatch); i++)
-			truncate_cleanup_folio(fbatch.folios[i]);
+		for (i = 0; i < folio_batch_count(&fbatch); i++) {
+			folio = fbatch.folios[i];
+			duplication_eviction(folio, true);
+			truncate_cleanup_folio(folio);
+		}
 		delete_from_page_cache_batch(mapping, &fbatch);
-		for (i = 0; i < folio_batch_count(&fbatch); i++)
+		for (i = 0; i < folio_batch_count(&fbatch); i++) {
 			folio_unlock(fbatch.folios[i]);
+		}
 		folio_batch_release(&fbatch);
 		cond_resched();
 	}
@@ -394,6 +402,7 @@ void truncate_inode_pages_range(struct address_space *mapping,
 				continue;
 
 			folio_lock(folio);
+			duplication_eviction(folio, true);
 			VM_BUG_ON_FOLIO(!folio_contains(folio, indices[i]), folio);
 			folio_wait_writeback(folio);
 			truncate_inode_folio(mapping, folio);
@@ -644,6 +653,8 @@ int invalidate_inode_pages2_range(struct address_space *mapping,
 			VM_BUG_ON_FOLIO(!folio_contains(folio, indices[i]), folio);
 			folio_wait_writeback(folio);
 
+			duplication_eviction(folio, true);
+
 			if (folio_mapped(folio))
 				unmap_mapping_folio(folio);
 			BUG_ON(folio_mapped(folio));
diff --git a/mm/vmscan.c b/mm/vmscan.c
index 77d015d5db0c..51fcdbc97c87 100644
--- a/mm/vmscan.c
+++ b/mm/vmscan.c
@@ -10,8 +10,11 @@
  *  Multiqueue VM started 5.8.00, Rik van Riel.
  */
 
-#define pr_fmt(fmt) KBUILD_MODNAME ": " fmt
+#include "linux/export.h"
 
+#include "linux/page-flags.h"
+#include "linux/page_ref.h"
+#include "linux/stddef.h"
 #include <linux/mm.h>
 #include <linux/sched/mm.h>
 #include <linux/module.h>
@@ -57,6 +60,7 @@
 #include <linux/rculist_nulls.h>
 #include <linux/random.h>
 #include <linux/mmu_notifier.h>
+#include <linux/duplication.h>
 
 #include <asm/tlbflush.h>
 #include <asm/div64.h>
@@ -723,8 +727,13 @@ static int __remove_mapping(struct address_space *mapping, struct folio *folio,
 	int refcount;
 	void *shadow = NULL;
 
-	BUG_ON(!folio_test_locked(folio));
-	BUG_ON(mapping != folio_mapping(folio));
+	if (folio_test_twin(folio)) {
+		refcount = 1 + 1; // for struct duplication + current ref
+		if (!folio_ref_freeze(folio, refcount))
+			return 0;
+		duplication_eviction(folio, false);
+		return 1;
+	}
 
 	if (!folio_test_swapcache(folio))
 		spin_lock(&mapping->host->i_lock);
@@ -795,12 +804,15 @@ static int __remove_mapping(struct address_space *mapping, struct folio *folio,
 		if (reclaimed && folio_is_file_lru(folio) &&
 		    !mapping_exiting(mapping) && !dax_mapping(mapping))
 			shadow = workingset_eviction(folio, target_memcg);
+
 		__filemap_remove_folio(folio, shadow);
 		xa_unlock_irq(&mapping->i_pages);
 		if (mapping_shrinkable(mapping))
 			inode_add_lru(mapping->host);
 		spin_unlock(&mapping->host->i_lock);
 
+		duplication_eviction(folio, false);
+
 		if (free_folio)
 			free_folio(folio);
 	}
@@ -1328,6 +1340,15 @@ static unsigned int shrink_folio_list(struct list_head *folio_list,
 		if (folio_maybe_dma_pinned(folio))
 			goto activate_locked;
 
+		if (folio_test_main(folio) &&
+		    duplication_switch_main_eviction &&
+		    duplication_try_switch_main(folio, -1)) {
+			if (folio_put_testzero(folio))
+				goto free_it;
+			else
+				quick_print_folio(folio, true, "aled");
+		}
+
 		mapping = folio_mapping(folio);
 		if (folio_test_dirty(folio)) {
 			/*
@@ -1459,6 +1480,7 @@ static unsigned int shrink_folio_list(struct list_head *folio_list,
 
 		if (folio_test_anon(folio) && !folio_test_swapbacked(folio)) {
 			/* follow __remove_mapping for reference */
+			BUG_ON(folio_test_twin(folio) || folio_test_main(folio));
 			if (!folio_ref_freeze(folio, 1))
 				goto keep_locked;
 			/*
diff --git a/mm/vmstat.c b/mm/vmstat.c
index 3f4134423912..a9c936445b4a 100644
--- a/mm/vmstat.c
+++ b/mm/vmstat.c
@@ -1464,6 +1464,7 @@ const char * const vmstat_text[] = {
 #endif
 #endif
 #endif /* CONFIG_VM_EVENT_COUNTERS || CONFIG_MEMCG */
+	"twins",
 };
 #endif /* CONFIG_PROC_FS || CONFIG_SYSFS || CONFIG_NUMA || CONFIG_MEMCG */
 
diff --git a/mm/workingset.c b/mm/workingset.c
index a2b28e356e68..d89c650764b0 100644
--- a/mm/workingset.c
+++ b/mm/workingset.c
@@ -253,6 +253,9 @@ static void *lru_gen_eviction(struct folio *folio)
 	hist = lru_hist_from_seq(min_seq);
 	atomic_long_add(delta, &lrugen->evicted[hist][type][tier]);
 
+	BUG_ON(!memcg);
+	BUG_ON(!pgdat);
+
 	return pack_shadow(mem_cgroup_id(memcg), pgdat, token, refs);
 }
 
